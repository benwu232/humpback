{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is my attempt to use martin's data creation technique and then put it in Siamene Network using Fast AI. I am using following links for this:\n",
    "https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563/output\n",
    "https://github.com/radekosmulski/whale/blob/master/siamese_network_prototype.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress annoying stderr output when importing keras.\n",
    "import sys\n",
    "import platform\n",
    "old_stderr = sys.stderr\n",
    "sys.stderr = open('/dev/null' if platform.system() != 'Windows' else 'nul', 'w')\n",
    "\n",
    "sys.stderr = old_stderr\n",
    "\n",
    "import random\n",
    "from scipy.ndimage import affine_transform\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "# Determise the size of each image\n",
    "from os.path import isfile\n",
    "from PIL import Image as pil_image\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path('../input')\n",
    "train_path = root_path/'train_224'\n",
    "test_path = root_path/'test_224'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'siamese_resnet50_224'\n",
    "\n",
    "#arch = models.resnet50\n",
    "SZ = 224\n",
    "BS = 24\n",
    "NUM_WORKERS = 6\n",
    "SEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(root_path/'train.csv')\n",
    "im_count = df[df.Id != 'new_whale'].Id.value_counts()\n",
    "im_count.name = 'sighting_count'\n",
    "df = df.join(im_count, on='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training data, we are taking all the images except for the category - New Whale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data for training  (15697, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wb/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000e88ab.jpg</td>\n",
       "      <td>w_f48451c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001f9222.jpg</td>\n",
       "      <td>w_c3d896a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Image         Id\n",
       "0  0000e88ab.jpg  w_f48451c\n",
       "1  0001f9222.jpg  w_c3d896a"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = df[df['Id']!= 'new_whale']\n",
    "#new_df = new_df[new_df['sighting_count']>1]\n",
    "\n",
    "print('shape of data for training ',new_df.shape)\n",
    "new_df.drop(columns = ['sighting_count'] , inplace=True)\n",
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wb/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15694</th>\n",
       "      <td>fff7faf61.jpg</td>\n",
       "      <td>w_9cf0388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15695</th>\n",
       "      <td>fff9002e0.jpg</td>\n",
       "      <td>w_bd1c3d5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15696</th>\n",
       "      <td>fffcde6fe.jpg</td>\n",
       "      <td>w_9f30885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Image         Id\n",
       "15694  fff7faf61.jpg  w_9cf0388\n",
       "15695  fff9002e0.jpg  w_bd1c3d5\n",
       "15696  fffcde6fe.jpg  w_9f30885"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.reset_index(inplace=True)\n",
    "new_df.drop(columns='index' , inplace=True)\n",
    "new_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.array([(    0, '0000e88ab.jpg', 'w_f48451c'), (    1, '0001f9222.jpg', 'w_c3d896a'),\n",
       "           (    2, '00029d126.jpg', 'w_20df2c5'), (    3, '000a6daec.jpg', 'w_dd88965'), ...,\n",
       "           (15693, 'ffef89eed.jpg', 'w_9c506f6'), (15694, 'fff7faf61.jpg', 'w_9cf0388'),\n",
       "           (15695, 'fff9002e0.jpg', 'w_bd1c3d5'), (15696, 'fffcde6fe.jpg', 'w_9f30885')],\n",
       "          dtype=[('index', '<i8'), ('Image', 'O'), ('Id', 'O')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.to_records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The data set we are using for training contains all images except of new whales.\n",
    "we don't require creating phase values for this  datasets , as duplicate images are very few.\n",
    "i am using index present in train.csv as the phase value as we can use it for indexing very easily \n",
    "\"\"\"\n",
    "\n",
    "tagged = dict([(p,w) for _,p,w in new_df.to_records()])\n",
    "h2ps = dict([(idx , p ) for   idx,p,w in new_df.to_records()])\n",
    "p2h   = dict([(p , idx) for idx , p , w in new_df.to_records()])\n",
    "h2p = h2ps.copy()\n",
    "join = tagged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6de0c6b150b49669f817b24c46d9f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15697), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15697,\n",
       " [('0000e88ab.jpg', (224, 224)),\n",
       "  ('0001f9222.jpg', (224, 224)),\n",
       "  ('00029d126.jpg', (224, 224)),\n",
       "  ('000a6daec.jpg', (224, 224)),\n",
       "  ('0016b897a.jpg', (224, 224))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_path(p):\n",
    "    if isfile(train_path/p): \n",
    "        return train_path/p\n",
    "    if isfile(test_path/p): \n",
    "        return test_path/p\n",
    "    return p\n",
    "\n",
    "p2size = {}\n",
    "for p in tqdm_notebook(join):\n",
    "    size      = pil_image.open(expand_path(p)).size\n",
    "    p2size[p] = size\n",
    "len(p2size), list(p2size.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15697"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## phase value for all categories except new whale\n",
    "h2ws = {}\n",
    "new_whale = 'new_whale'\n",
    "for p,w in tagged.items():\n",
    "    if w != new_whale: # Use only identified whales\n",
    "        h = p2h[p]\n",
    "        if h not in h2ws: h2ws[h] = []\n",
    "        if w not in h2ws[h]: h2ws[h].append(w)\n",
    "for h,ws in h2ws.items():\n",
    "    if len(ws) > 1:\n",
    "        h2ws[h] = sorted(ws)\n",
    "len(h2ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5004"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## for each whale category, observe the associated phase values , \n",
    "##store all whale categories even the categories with just one image ( this is  different from martin's approach)\n",
    "\n",
    "w2hs = {}\n",
    "for h,ws in h2ws.items():\n",
    "    if len(ws) == 1: # Use only unambiguous pictures\n",
    "\n",
    "        w = ws[0]\n",
    "        if w not in w2hs: w2hs[w] = []\n",
    "        if h not in w2hs[w]: w2hs[w].append(h)\n",
    "for w,hs in w2hs.items():\n",
    "    #if len(hs) > 1:\n",
    "        w2hs[w] = sorted(hs)\n",
    "len(w2hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15697"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(h2ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_image(p):\n",
    "    img = pil_image.open(expand_path(p))\n",
    "    return img\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [] # A list of  indices of images to be used in training data.\n",
    "for hs in w2hs.values():\n",
    "    if len(hs) >= 1:\n",
    "        train += hs\n",
    "random.shuffle(train)\n",
    "train_set = set(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15697, 5004)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we have whales categories with phases(images) more than 1. shuffle the phase values now.\n",
    "w2ts = {} #Associate the image index from train to each whale id.\n",
    "for w,hs in w2hs.items():\n",
    "    for h in hs:\n",
    "        if h in train_set:\n",
    "            if w not in w2ts: w2ts[w] = []\n",
    "            if h not in w2ts[w]: w2ts[w].append(h)\n",
    "for w,ts in w2ts.items(): w2ts[w] = np.array(ts)\n",
    "## then again for each whale categories see how many images you have , \n",
    "## you are working with 5004 whale categories and 15697 images \n",
    "    \n",
    "    \n",
    "t2i = {} # The position in train of each training image id\n",
    "for i,t in enumerate(train): t2i[t] = i\n",
    "\n",
    "len(train),len(w2ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import Sequence\n",
    "# import keras\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "import random\n",
    "#from keras import backend as K\n",
    "\n",
    "try:\n",
    "    from lapjv import lapjv\n",
    "    segment = False\n",
    "except ImportError:\n",
    "    print('Module lap not found, emulating with much slower scipy.optimize.linear_sum_assignment')\n",
    "    segment = True\n",
    "    from scipy.optimize import linear_sum_assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import functions from fast ai library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from fastai.vision import *\n",
    "from fastai.metrics import accuracy_thresh\n",
    "from fastai.basic_data import *\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "from fastai.callbacks.hooks import num_features_model, model_sizes\n",
    "from fastai.layers import BCEWithLogitsFlat\n",
    "from fastai.basic_train import Learner\n",
    "from skimage.util import montage\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "import re\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "from functional import seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fn2label = {row[1].Image: row[1].Id for row in df.iterrows()}  #new_\n",
    "path2fn = lambda path: re.search('\\w*\\.jpg$', path).group(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataset for all the training images. Because of some reason , i am not able to create validation set as well ( produces error while indexing from match and unmatch matrices. If someone is able to find the work arounf the help will be appreciated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df.Id.unique()\n",
    "data = (\n",
    "    ImageItemList  ##df[(df.Id != 'new_whale') & (df.sighting_count >1)]\n",
    "        .from_df( df[(df.Id != 'new_whale')], train_path, cols=['Image'])\n",
    "        .no_split()##split_by_valid_func(lambda path: path2fn(path) in val_fns) \n",
    "        .label_from_func(lambda path: fn2label[path2fn(path)] ,  classes=classes)\n",
    "        .add_test(ImageItemList.from_folder(test_path))\n",
    "        .transform(get_transforms(do_flip=False), size=SZ, resize_method=ResizeMethod.SQUISH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15697\n",
      "15697\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(data.train.x))\n",
    "print(len(data.valid.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "import random\n",
    "\n",
    "# First try to use lapjv Linear Assignment Problem solver as it is much faster.\n",
    "# At the time I am writing this, kaggle kernel with custom package fail to commit.\n",
    "# scipy can be used as a fallback, but it is too slow to run this kernel under the time limit\n",
    "# As a workaround, use scipy with data partitioning.\n",
    "# Because algorithm is O(n^3), small partitions are much faster, but not what produced the submitted solution\n",
    "try:\n",
    "    from lapjv import lapjv\n",
    "    segment = False\n",
    "except ImportError:\n",
    "    print('Module lap not found, emulating with much slower scipy.optimize.linear_sum_assignment')\n",
    "    segment = True\n",
    "    from scipy.optimize import linear_sum_assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TwoImDataset creation is the part where I am trying to replicate 'TrainingData Class' from https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563/output\n",
    "For whale categories having just one images in training data , matching pair -  same image pair (A,A) . For other categories it creates a de arrangement.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7610915632825416"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "def is_even(num): return num % 2 == 0\n",
    "\n",
    "class TwoImDataset(Dataset):\n",
    "    def __init__(self, ds, score, steps = 1000):\n",
    "        self.ds = ds\n",
    "        self.whale_ids = ds.y.items\n",
    "        self.steps =1000\n",
    "        self.score  = -score\n",
    "        for ts in w2ts.values():\n",
    "            idxs =  ts.copy() #[t2i[t] for t in ts]\n",
    "            #idxs = [i for i in  idxs if i <score.shape[0]]\n",
    "            for i in idxs:\n",
    "                for j in idxs:\n",
    "                    self.score[i,j] = 10000.0   # Set a large value for matching\n",
    "        self.epsilon = 1.0\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def set_epsilon(self, epsilon=1.0):\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 2 * len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prob = np.random.random()\n",
    "        choice = idx % 2 == 0\n",
    "        if choice:\n",
    "            idx //=2\n",
    "        else:\n",
    "            idx = (idx - 1) // 2\n",
    "        \n",
    "        if prob < self.epsilon: \n",
    "            return self.sample_random(idx, choice)\n",
    "        else:\n",
    "            return self.sample_policy(idx, choice)\n",
    "                \n",
    "    def sample_random(self, idx, choice):\n",
    "        if choice:\n",
    "            return self.sample_same(idx)\n",
    "        else: return self.sample_different(idx)\n",
    "\n",
    "    def sample_same(self, idx):\n",
    "        whale_id = self.whale_ids[idx]        \n",
    "        candidates = list(np.where(self.whale_ids == whale_id)[0])\n",
    "        candidates.remove(idx) # dropping our current whale - we don't want to compare against an identical image!\n",
    "        \n",
    "        if len(candidates) == 0: # oops, there is only a single whale with this id in the dataset\n",
    "            return self.sample_different(idx)\n",
    "        \n",
    "        np.random.shuffle(candidates)\n",
    "        return self.construct_example(self.ds[idx][0], self.ds[candidates[0]][0], 1)\n",
    "    \n",
    "    def sample_different(self, idx):\n",
    "        whale_id = self.whale_ids[idx]\n",
    "        candidates = list(np.where(self.whale_ids != whale_id)[0])\n",
    "        np.random.shuffle(candidates)\n",
    "        return self.construct_example(self.ds[idx][0], self.ds[candidates[0]][0], 0)\n",
    "  \n",
    "    def sample_policy(self, idx , tag):\n",
    "        #set_trace()\n",
    "        if tag==0:\n",
    "            first_image_id =  self.match[idx][0]\n",
    "            second_image_id = self.match[idx][1]\n",
    "            #if first_image_id < len(self.ds) and second_image_id< len(self.ds):         \n",
    "            return self.construct_example(self.ds[first_image_id][0], self.ds[second_image_id][0], 1)\n",
    "        else:\n",
    "            first_image_id =  self.unmatch[idx][0]\n",
    "            second_image_id = self.unmatch[idx][1]     \n",
    "            return self.construct_example(self.ds[first_image_id][0], self.ds[second_image_id][0], 0)\n",
    "  \n",
    "    def on_epoch_end(self):\n",
    "        if self.steps <= 0: return # Skip this on the last epoch.\n",
    "        self.steps     -= 1\n",
    "        self.match      = []\n",
    "        self.unmatch    = []\n",
    "        if segment:\n",
    "            tmp   = []\n",
    "            batch = 512\n",
    "            for start in range(0, score.shape[0], batch):\n",
    "                end = min(score.shape[0], start + batch)\n",
    "                _, x = linear_sum_assignment(self.score[start:end, start:end])\n",
    "                tmp.append(x + start)\n",
    "            x = np.concatenate(tmp)\n",
    "        else:\n",
    "            #print('using lapjv')\n",
    "            x,_,_ = lapjv(self.score) # Solve the linear assignment problem\n",
    "        y = np.arange(len(x),dtype=np.int32)\n",
    "\n",
    "        # Compute a derangement for matching whales\n",
    "        for ts in w2ts.values():\n",
    "            d = ts.copy()\n",
    "            if (len(d)==1):\n",
    "                for ab in zip(ts,d): self.match.append(ab)\n",
    "            else:                \n",
    "                while True:\n",
    "                    random.shuffle(d)\n",
    "                    if not np.any(ts == d): break\n",
    "                for ab in zip(ts,d): self.match.append(ab)\n",
    "\n",
    "        # Construct unmatched whale pairs from the LAP solution.\n",
    "        for i,j in zip(x,y):\n",
    "            if i == j:\n",
    "                print(self.score)\n",
    "                print(x)\n",
    "                print(y)\n",
    "                print(i,j)\n",
    "            assert i != j\n",
    "            self.unmatch.append((train[i],train[j]))\n",
    "\n",
    "        # Force a different choice for an eventual next epoch.\n",
    "        self.score[x,y] = 10000.0\n",
    "        self.score[y,x] = 10000.0\n",
    "        random.shuffle(self.match)\n",
    "        random.shuffle(self.unmatch)\n",
    "        print('end of epoch, math',self.match[0][0])\n",
    "        print('end of epoch, unmatch',self.unmatch[0][0])\n",
    "        \n",
    "        print(len(self.match), len(train), len(self.unmatch), len(train))\n",
    "        #assert len(self.match) == len(train) and len(self.unmatch) == len(train)\n",
    "    \n",
    "    def construct_example(self, im_A, im_B, class_idx):\n",
    "        return [im_A, im_B], class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of epoch, math 11983\n",
      "end of epoch, unmatch 10762\n",
      "15697 15697 15697 15697\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create a 2D score matrix of size of training data\n",
    "\"\"\"\n",
    "\n",
    "score = np.random.random_sample(size=(len(train),len(train)))\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    TwoImDataset(data.train , score),\n",
    "    batch_size=BS,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_batch(batch):\n",
    "    stat_tensors = [torch.tensor(l).cuda() for l in imagenet_stats]\n",
    "    return [normalize(batch[0][0], *stat_tensors), normalize(batch[0][1], *stat_tensors)], batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bunch = ImageDataBunch(train_dl , train_dl) ##, valid_dl\n",
    "data_bunch.add_tfm(normalize_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The netowrk architecture is also inspired from Martin's notebook (part after we extract features for two image pairs)\n",
    "\"\"\"\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, arch=models.resnet50):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.cnn = create_body(arch)\n",
    "        self.head = nn.Linear(num_features_model(self.cnn), 1)  #\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1 , 32 , kernel_size= (1 , 4) , padding = 0 ,stride=1)\n",
    "        self.conv2 = nn.Conv2d( 1 , 1 , kernel_size = (32 ,1 ) , padding = 0  , stride=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, im_A, im_B):\n",
    "        x1, x2 = seq(im_A, im_B).map(self.cnn).map(self.process_features)\n",
    "        d1 = self.calculate_distance(x1, x2)\n",
    "        d2 = (x1 + x2)\n",
    "        d3 = (x1*x2)\n",
    "        d4 = (x1-x2)*(x1 - x2)\n",
    "        concat_layer = torch.cat([d1 ,d2,d3, d4]  ,dim = 1)\n",
    "        concat_layer = concat_layer.view( - 1, 1, num_features_model(self.cnn) , 4)   ## no of channels is second dimension\n",
    "        concat_layer  = F.relu(self.conv1(concat_layer))\n",
    "        concat_layer = concat_layer.view(-1 ,1,32, num_features_model(self.cnn))\n",
    "        concat_layer = F.relu(self.conv2(concat_layer))\n",
    "        concat_layer_fn = concat_layer.view(-1 ,num_features_model(self.cnn))\n",
    "        dropt = self.dropout(concat_layer_fn)\n",
    "        out = self.head(dropt)\n",
    "        return out\n",
    "    \n",
    "    def process_features(self, x): \n",
    "        y = x.reshape(*x.shape[:2], -1)\n",
    "        return x.reshape(*x.shape[:2], -1).max(-1)[0]\n",
    "    def calculate_distance(self, x1, x2): return (x1 - x2).abs_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data_bunch, SiameseNet(emb_len=256, drop_rate=0.5), \n",
    "                loss_func=ContrastiveLoss(margin=10), \n",
    "                wd=0.01,\n",
    "                metrics=[lambda preds, targs: accuracy_thresh(preds.squeeze(), targs, sigmoid=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(step, pars):\n",
    "    \"Linearly output value, end_step must greater than start_step\"\n",
    "    start_value = pars[0]\n",
    "    end_value = pars[1]\n",
    "    start_step = pars[2]\n",
    "    end_step = pars[3]\n",
    "    assert start_step <= end_step\n",
    "\n",
    "    if step < start_step:\n",
    "        return start_value\n",
    "    elif step >= end_step:\n",
    "        return end_value\n",
    "    return start_value - (step - start_step) * (start_value - end_value) / (end_step - start_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_decay = partial(linear_schedule, pars=(1.0, 0.05, 2, 12))\n",
    "linear_decay(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataloaderCallback(fastai.callbacks.tracker.TrackerCallback):\n",
    "    def __init__(self, learn, schedule_pars=(1.0, 0.05, 0, 10)):\n",
    "        super().__init__(learn)\n",
    "        self.schedule = partial(linear_schedule, pars=schedule_pars)\n",
    "        \n",
    "    #def on_batch_end(self, last_loss, epoch, num_batch, **kwargs: Any) -> None:\n",
    "    def on_epoch_begin(self, epoch, **kwargs: Any) -> None:\n",
    "        epsilon = self.schedule(epoch)\n",
    "        self.learn.data.train_ds.set_epsilon(epsilon)\n",
    "        self.learn.data.train_ds.on_epoch_end()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.split([learn.model.cnn[:6], learn.model.cnn[6:], learn.model.fc1])\n",
    "learn.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xl8XHW9//HXJ3vSrG2SNm0K6UbpvhAKiCgWRVYVccGHenFffuK+X3+/e11+XndRrw+9lysC/gSVTRFEAbGAILake+m+NyFN0qTZl5nMfH9/zCnEmrRpMmfOZOb9fDzm0ZkzZ+Z8vpl03jnf7znfY845REQkfWUEXYCIiARLQSAikuYUBCIiaU5BICKS5hQEIiJpTkEgIpLmFAQiImlOQSAikuYUBCIiaS4r6AJGo7y83NXU1ARdhojIhLJ+/fpjzrmK0603IYKgpqaGurq6oMsQEZlQzOzQaNZT15CISJpTEIiIpDkFgYhImlMQiIikOQWBiEiaUxCIiKQ5BYGISJqbEOcRiIhMdE/ubmHT4XaizuEAnGNKYS5XLp5GZXFeoLUpCEREfHSkrZevPPg8f97RPOzzX3nweV4+r4LrVkzntYumUZCT+K9lBYGIiA/6wxH++8n9/OSJvWRmGP961bnc+LIasjMyMAMzY29zN7/b2MBvNzbwyd9sprRgO/d+6CLmVhYltFZzziV0g2NRW1vrNMWEiEwEHX1h7n7uCLf/7SAN7X1cs7SKL129gKqS/BFfE4061h1s46a7NlBakMMDH7mYSbnj/zvdzNY752pPt572CERE4uBway8/f+YA99QdoScU4YJZk/n2m5Zy8dzy0742I8O4cPYUfnTDCt5x61q+eP9WfnjDcswsAZUrCERExq2la4Arf/gUoUiUa5dO5z0vn8XiGSVn/D4vm1vOpy+fz3ce2UVtTRn/clFN/IsdhoJARGScHtjUQE8owh8+9nIWTT/zABjqw6+cw4ZDx/naQ9tZPKOElWeVxanKkek8AhGRcbp/QwPLqkvGHQIQ6yb6/luWM7U4j5vu3EBbTygOFZ5mm75vQUQkhe1o7GR7YydvXFkdt/csKcjmp28/j+6BQbY2dMTtfUeiriERkXH47cYGsjKMa5dNj+v7Lqku4ZkvrKYoLzuu7zsc7RGIiIxRJOr43cYGLp1fyeRJOXF//0SEACgIRETG7Jm9x2juGuD6lTOCLmVcFAQiImN0/4Z6ivOyWL2gMuhSxkVBICIyBt0Dg/zp+aNcu2w6uVmZQZczLgoCEZEx+OPWRvrD0bgeLRQUBYGIyBjcv6GBmikFrDyrNOhSxk1BICJyhuqP9/Ls/lauW1GdsPmA/KQgEBE5Q7c/c5DMDOP68yb20UIn+B4EZpZpZhvN7CHv8SwzW2tme8zsN2YW/4NvRUR80tEX5lfrDnPN0iqqywqCLicuErFH8HFgx5DH3wJuds7NA44D701ADSIicXHn2kP0hCJ84BWzgy4lbnwNAjOrBq4GfuY9NmA1cK+3yh3AG/ysQUQkXvrDEW575iCXzCuPywRzycLvPYIfAJ8Dot7jKUC7c27Qe1wPpEYnm4ikvN9tbKCla4APvXJO0KXElW9BYGbXAM3OufVDFw+z6rDXyjSzD5hZnZnVtbS0+FKjiMhoRaOOW57az6LpxbxszpSgy4krP/cILgZeZ2YHgV8T6xL6AVBqZidmPa0GXhjuxc65W5xztc652oqKCh/LFBE5vcd2NLH/WA8ffOWclDhkdCjfgsA590XnXLVzrga4AfiLc+7twBrgTd5qNwIP+FWDiEi83PLUfqrL8rlq8bSgS4m7IM4j+DzwKTPbS2zM4NYAahARGbV1B9pYf+g4779kNlmZqXf6VUIuTOOcewJ4wru/H1iViO2KiIzXYCTKl3//PFOLc3lz7cSfV2g4qRdtIiJxdMezh9je2Mm/X7uIgpzUvKijgkBEZASNHX18/9FdXDq/gitTcGzgBAWBiMgIvvL77QxGHV97/eKUO1JoKAWBiMgwHt/RxJ+eP8rHLpvHzMmpMafQSBQEIiIn6Q0N8m8PPM+8ykLef0nqzCk0ktQc+RARGYefrNlHQ3sfd3/wInKyUv/v5dRvoYjIGWju6ufWpw/wumXTWTVrctDlJISCQERkiJ+s2UcoEuVTrzkn6FISRkEgIuKpP97LnWsP8ZbamdSUTwq6nIRREIiIeH745z2YGR+7bG7QpSSUgkBEBNjb3M19G+r5lwvPpqokP+hyEkpBICIC3PzYbvKzM/nwpal10ZnRUBCISNrb1tDBH7Y28t5LZjOlMDfochJOQSAiae9Hj++htCCb910yK+hSAqEgEJG01tEXZs2uZt58XjXFedlBlxMIBYGIpLU/b28iHHFctaQq6FICoyAQkbT28NZGZpTms3xmadClBEZBICJpq7M/zF/3HOPKxdNSeprp01EQiEjaenxHE6FIlKuWpm+3ECgIRCSN/WHLUaaX5LEijbuFQEEgImmqqz/MU3tauGJxVVp3C4GCQETS1OM7mgkNRrl6aepei3i0FAQikpb+sLWRacV5rJhZFnQpgfMtCMwsz8zWmdlmM3vezL7iLb/dzA6Y2SbvttyvGkREhtPVH+bJ3S1cuWQaGRnp3S0E/l6qcgBY7ZzrNrNs4Gkz+6P33Gedc/f6uG0RkRH9ZafXLZTGJ5EN5VsQOOcc0O09zPZuzq/tiYiM1h+2xLqFVp6lbiHweYzAzDLNbBPQDDzmnFvrPfV1M9tiZjebWfpN9ScigQkNRnlqTwuXL5qqbiGPr0HgnIs455YD1cAqM1sMfBE4FzgfmAx8frjXmtkHzKzOzOpaWlr8LFNE0sjWhg76w1FeNmdK0KUkjYQcNeScaweeAK5wzjW6mAHgNmDVCK+5xTlX65yrraioSESZIpIGnjvYBsD5NZMDriR5+HnUUIWZlXr384FXAzvNrMpbZsAbgG1+1SAicrJ1B9qYUzEpLS9AMxI/jxqqAu4ws0xigXO3c+4hM/uLmVUABmwCPuRjDSIiL4pEHc8dbOOapdODLiWp+HnU0BZgxTDLV/u1TRGRU9l5tJOu/kEumKVuoaF0ZrGIpI11B2LjA6sUBP9AQSAiaWPdgTaqy/KZXpofdClJRUEgImnBOce6A23aGxiGgkBE0sK+lh5ae0Ks0mGj/0RBICJp4cT5A9oj+GcKAhFJC+sOtFFemMus8klBl5J0FAQikhbWHWjjglmT0/5qZMNREIhIyqs/3ktDe5+6hUagIBCRlHfi/AHNLzQ8BYGIpLx1B9oozsti/rSioEtJSgoCEUl56w60cX7NZDJ1/YFhKQhEJKU1tPex/1iPxgdOQUEgIintdxsbALhysa5PPBIFgYikLOcc922oZ1XNZM6aUhB0OUlLQSAiKWtzfQf7W3q4/rwZQZeS1BQEIpKy7t9QT25WBlcuUbfQqSgIRCQlhQaj/H7zC1y+aBrFedlBl5PUFAQikpLW7GqmvTfMG1eqW+h0FAQikpLuW19PRVEul8wtD7qUpKcgEJGU09YTYs2uZt6wfDpZmfqaOx39hEQk5Ty05QXCEccbV1YHXcqEoCAQkZRz34YGFlQVs6CqOOhSJgQFgYiklP0t3Ww+0s71GiQeNd+CwMzyzGydmW02s+fN7Cve8llmttbM9pjZb8wsx68aRCT9nLgk5WULpgZcycTh5x7BALDaObcMWA5cYWYXAt8CbnbOzQOOA+/1sQYRSTNb6jsoysuiRlNKjJpvQeBiur2H2d7NAauBe73ldwBv8KsGEUk/2xo6WDKjRJekPAO+jhGYWaaZbQKagceAfUC7c27QW6UeGLYjz8w+YGZ1ZlbX0tLiZ5kikiJCg1F2NHaxZEZJ0KVMKL4GgXMu4pxbDlQDq4AFw602wmtvcc7VOudqKyoq/CxTRFLE7qYuQpEoS6oVBGciIUcNOefagSeAC4FSM8vynqoGXkhEDSKS+rY2dACwdEZpwJVMLH4eNVRhZqXe/Xzg1cAOYA3wJm+1G4EH/KpBRNLLlvoOSvKzmTk5P+hSJpSs068yZlXAHWaWSSxw7nbOPWRm24Ffm9n/BTYCt/pYg4ikEQ0Uj41vQeCc2wKsGGb5fmLjBSIicTMwGGHn0U7e+/LZQZcy4Yyqa8jM5phZrnf/UjP72IluHxGRZLD7aDfhiGOpBorP2GjHCO4DImY2l1hXzizgLt+qEhE5Q1sa2gF06OgYjDYIot6x/9cBP3DOfZLYGICISFLY1tBBaUE21WUaKD5Tow2CsJm9jdhRPg95y3TtNxFJGlvqNVA8VqMNgncDFwFfd84dMLNZwC/9K0tEZPT6wxF2N+mM4rEa1VFDzrntwMcAzKwMKHLOfdPPwkRERmvX0S4NFI/DaI8aesLMis1sMrAZuM3Mvu9vaSIio3PijOLF2iMYk9F2DZU45zqBNwK3OefOI3amsIhI4LbWdzB5Ug4zSjVQPBajDYIsM6sC3sJLg8UiIklhS0MHizVQPGajDYKvAo8A+5xzz5nZbGCPf2WJiIxOfzjCnqYulqpbaMxGO1h8D3DPkMf7gev9KkpEZLR2NHYyGHUaHxiH0Q4WV5vZb82s2cyazOw+M6v2uzgRkdM5MVCsaxCM3Wi7hm4Dfg9MJ3ZFsQe9ZSIigdp0uJ2Kolyml+QFXcqENdogqHDO3eacG/RutwO6bJiIBG7TkXaWzyzVQPE4jDYIjpnZO7xrEGea2TuAVj8LExE5nY7eMPuP9bB8piZDHo/RBsF7iB06ehRoJHaFsXf7VZSIyGhsqo/NOLpCQTAuowoC59xh59zrnHMVzrlK59wbiJ1cJiISmE2H2zHTQPF4jeeaxZ+KWxUiImOw6chx5lUWUpSnyZDHYzxBoJEZEQmMc+7FgWIZn/EEgYtbFSIiZ+hwWy/He8Msn1kWdCkT3inPLDazLob/wjdAszuJSGA2HYkNFGuPYPxOGQTOuaJEFSIiciY2Hm4nPzuTc6YWBl3KhDeerqFTMrOZZrbGzHaY2fNm9nFv+ZfNrMHMNnm3q/yqQURS16Yj7SypLiEr07evsbQxqknnxmgQ+LRzboOZFQHrzewx77mbnXPf9XHbIpLCBgYjbH+hk3ddXBN0KSnBtyBwzjUSO/kM51yXme0gNk+RiMi47GjsIhSJanwgThKyT2VmNcAKYK236CYz22JmP/eugSwiMmqbDh8HNFAcL74HgZkVAvcBn/Aud/lTYA6wnNgew/dGeN0HzKzOzOpaWlr8LlNEJpBNR9qpLMqlSjOOxoWvQWBm2cRC4E7n3P0Azrkm51zEORcF/gdYNdxrnXO3OOdqnXO1FRWa6FREXqIZR+PLz6OGDLgV2OGc+/6Q5VVDVrsO2OZXDSKSeo73hDjY2svys9QtFC9+HjV0MfBOYKuZbfKW/SvwNjNbTuxEtYPAB32sQURSzIkZRzU+ED9+HjX0NMPPR/SwX9sUkdR3YsbRpdUKgnjRmRgiMmE453h4ayMrZpZSmOtnh0Z6URCIyISx6Ug7e5q7eUvtzKBLSSkKAhGZMO6uO0J+diZXL606/coyagoCEZkQekODPLi5kauXVulCNHGmIBCRCeHhrUfpHhhUt5APFAQiMiHcXXeEWeWTOL9Gs9LEm4JARJLegWM9rDvQxptrq3U2sQ8UBCKS9O6pO0KGwfUrq4MuJSUpCEQkqQ1Goty3oZ5L51cytViTzPlBQSAiSe2pPS00dQ5okNhHCgIRSWr31NUzZVIOq8+tDLqUlKUgEJGk1TMwyF92NnPN0ipysvR15Rf9ZEUkaT2xq4WBwShXLtGZxH5SEIhI0np4WyPlhTmcXzM56FJSmoJARJJSfzjCmp3NXL5oGpkZOnfATwoCEUlKT+5uoTcU4crF04IuJeUpCEQkKf1p21FK8rO5cPaUoEtJeQoCEUk6A4MR/ryjicsXTiU7U19TftNPWESSzt/2ttLVP8iVS9QtlAgKAhFJOn/c1khRbhYXzy0PupS0oCAQkaQSjkR5dHsTly2oJDcrM+hy0oKCQESSytr9bbT3hrlisU4iSxQFgYgklYe3NVKQk8ml8yuCLiVt+BYEZjbTzNaY2Q4ze97MPu4tn2xmj5nZHu9fXW5IRIDY0UKPbDvKq+ZXkpetbqFE8XOPYBD4tHNuAXAh8BEzWwh8AXjcOTcPeNx7LCLCAxtfoLUnxA2rNOV0IvkWBM65RufcBu9+F7ADmAG8HrjDW+0O4A1+1SAiE4dzjp89vZ9zpxXxch0tlFAJGSMwsxpgBbAWmOqca4RYWACaZFxEeHJ3C7ubunnfJbN1XeIE8z0IzKwQuA/4hHOu8wxe9wEzqzOzupaWFv8KFJGk8LO/HqCyKJfXLZsedClpx9cgMLNsYiFwp3Pufm9xk5lVec9XAc3DvdY5d4tzrtY5V1tRoaMHRFLZ9hc6eXrvMW58WY0uQBMAP48aMuBWYIdz7vtDnvo9cKN3/0bgAb9qEJGJ4WdP76cgJ5O3X3BW0KWkpSwf3/ti4J3AVjPb5C37V+CbwN1m9l7gMPBmH2sQkSTX1NnPg5tf4O0XnE1pQU7Q5aQl34LAOfc0MNKIz2V+bVdEJpbb/3aQSNTxnotnBV1K2lJnnIgEpq0nxF1rD/PaRdM4a0pB0OWkLQWBiAQiEnV87Fcb6QtH+OjqeUGXk9YUBCISiJsf283Te4/xtdcvYuH04qDLSWsKAhFJuMe2N/HjNXu54fyZvPV8HSkUNAWBiCTUgWM9fOo3m1gyo4Qvv25R0OUICgIRSaDe0CAf/uV6MjONn75jpWYYTRJ+nkcgIvIi5xyfvWcLu5q6uP3dq6gu01FCyUJ7BCKSED95Yh9/2NrI5157Lq88R9PGJBMFgYj47i87m/juo7u4dtl0PvTK2UGXIydREIiIr/a1dPPxX21iYVUx375+qaaYTkIKAhHxTWd/mPf/oo7srAz++53nkZ+jweFkpMFiEfHNNx7ewaHWXu583wUaHE5i2iMQEV9sqW/n188d4caLarhw9pSgy5FTUBCISNxFo44v//55pkzK4ROv0TxCyU5BICJx99uNDWw43M7nrjiX4rzsoMuR01AQiEhcdfWH+cYfd7J8ZilvWlkddDkyChosFpG4+tHje2jtGeDWG2vJyNChohOB9ghEJG72Nndx2zMHeWvtTJbNLA26HBklBYGIxMW2hg7eddtzFORk8tnXzg+6HDkDCgIRGbd76o5w/U//RiTq+MV7L2BKYW7QJckZ0BiBiIzZwGCErz64nTvXHuZlc6bwn29boRCYgBQEIjIm0ajj/b9Yz1O7W/jgK2fz2cvnk5WpToaJSEEgImPy82cO8NTuFr76+kX8y0U1QZcj4+BbfJvZz82s2cy2DVn2ZTNrMLNN3u0qv7YvIv7Z3dTFtx/ZxasXTOWdF54ddDkyTn7ux90OXDHM8pudc8u928M+bl9EfBAajPKJX2+iKDeLb16/RNNKpwDfgsA59xTQ5tf7i0gwfvDn3Wxv7OSb1y+lXAPDKSGIkZ2bzGyL13VUNtJKZvYBM6szs7qWlpZE1iciI6g72MZ/PbmPG86fyWsWTg26HImTRAfBT4E5wHKgEfjeSCs6525xztU652orKnR9U5GgdfaH+eTdm6guK+B/X7Mw6HIkjhIaBM65JudcxDkXBf4HWJXI7YvI2Djn+Py9W2hs7+fmty6nMFcHHKaShAaBmVUNeXgdsG2kdUUkedzxt4P8cdtRPn/FuZx39og9ujJB+RbrZvYr4FKg3MzqgX8HLjWz5YADDgIf9Gv7IhIfW+rb+frDO3j1gkred8msoMsRH/gWBM65tw2z+Fa/tici8dfRF+Yjd22gsiiP7755mQ4VTVHq6BORYUWijs/du5nG9n7u/tBFlBbkBF2S+ERBICL/5Fj3AB//9Uae2dvK/756ASvP0rhAKlMQiMg/WHegjZvu2kBHX5hvv2kpb6mdGXRJ4jMFgYjgnKO1J8S96+v5ziO7mFmWz+3vXsXC6cVBlyYJoCAQSUOd/WGe3NXC3/e3sqe5mz1NXRzvDQNw1ZJpfOv6pRTlZQdcpSSKgkAkTTR39fOnbUd5bHsTf9/fSjjiKM7LYv60Iq5YXMW8ykIWVBVz4ezJOjoozSgIUkA06ujsDzMpN4vsES4MEhqMkp1p+g+eZpxzPLuvlV+uPcSjzzcxGHXMLp/Eey6exWsWTmXFWWVkZuh3It2ldBAcau3BOagpn5SwbQ5GorT2hNjb3M3upi72NHfT2N7HirPKWH1uJYumF5/Rl/GRtl5++Pge1h1oozg/i5L8bErzc8jNyuBoZz8N7X00tvcTikQBKM7Lorwwl7JJOfSHI7T3hjneG6I3FGHxjGL+7ZpFrJo12a/mSxKIRh3bGzt5cncL922oZ39LD6UF2bz74hreUjuTeVOLgi5Rkow554Ku4bRqa2tdXV3dGb/u03dv5r4N9bzinApuvOhsLp1fOa6/fqJRx13rDrPuQBv94Qj9g1H6wxF6Q4O094bp6AvT1T/4D68pzsuisjiPfS3dOAdVJXm86txKqsvyKcjOpCAni/ycTKYW5zGrfBLlhTmYGc2d/fx4zV5+te4wZsbq+ZWEIlHae0N09IXpD0eZVpLH9NJ8ZpTmU1GUS8/AIK3dAxzrCXG8J0RedialBdmUFeQwKSeTe9bX09jRz9VLqvjClecyc3LBmH8WEqzO/jB/3X2M7oEwURc75j8cibL5SDtP7z3Gse4QACvPKuUdF57NVUuqyMvODLhqSTQzW++cqz3teqkcBM2d/fxq3RHuXHuI5q4BqsvyefsFZ3Ptsiqqy87sS/BoRz+fuWczT+89xozSfIryssjNziQvK4NJuVmU5mdTnJ9NSX425YU5zKkoZO7UQioKczEzWroGWLOrmcd3NPHXPcfoDUWG3U5hbhY15QXsbe4mHHG89fyZfHT1XKpK8s+4/SfrC0W45an9/NeT+4g4x2sXTWNqUS6TC3OYMimHvOxMjveEaOsNc7wnRGgwypLqEmpryjinsogMdSH4oi8U4dn9sS/vuZWFzK0spHiYgdqBwQhrdrbwwKYGHt/ZTGgw+k/rTJmUwyXzynnFORW8fG45lcV5iWiCJCkFwRDhSJRHn2/iF88eZO2B2LVyls0s5eol07hiURUzJ+efsrvmj1sb+eJvtzIQjvJ/rlnI21bNHFdfu3OO/nCU3tAgvaEIvaEIjR19HDjWw8FjPRxo7WVqUS4fedVcX7q1Gjv6+N6ju3l2XyttPSH6wv8YSmZQkp+NwYtHkhTnZbHy7DLOnVbMvMpC5k0tZE5FIZOGzELpnKO9N0xDex8N7X280N5Hd/8gWZkZZGcaWRlGfk4mZQU5TJ4Uu1UW5yXVTJZd/WF2N3XTF4qQm51BblYGedmZ9IUisXYd76P+eC/He8NkZRq5WRnkZGaQn5PFvMpCFk4vZk5FITlZI8/nGIk6Dhzr5pm9razZ1cyz+1oZOOlLfVpxHjPK8glHYnud/eEord0D9IQiTJmUw7XLpnPtsiqmleSTaUZGBmSYMbkgR4EtL1IQjOBQaw8Pbz3Kw1sb2drQAUBOZgZTS3KZVpxHZXEeWRlGJOqIRGNfbM/ub2VpdQk/eOtyZlcUxqWOZNIXitDaM0B/OMrkSTmU5GeTmWE45zjc1kvdwePUHWpj4+F29rXE9lROGJqHY/1VWjyjmEvmVXDJvHLOO7uM3KxTd2F0Dwyy62gn2xu72NnYSWt3iKxMIzszg6wMozg/m/lTi1hQVcy8qYUvdon0hSI0d/XT0jVAW08odusN0dodYn9LN7ubumlo7zttvUW5WUwpzCEccYQiUUKDsVA/8XPJzjTmVBQyvTSf8sIcKopyKSvIoaG9j20NHWx/oZMeb49wdvkkLp1fyavOraC6rIB9zd3sbu5ib1M3Rzv7ycnKIC8rk7zsDIrysrlsQSUvn1tO1ggHBYgMpSAYhcOtvTyxu5mG9j6OdvRztCP2JRF1joyM2F+wmRkZXL5wKjetnjviETnpZDAS5VBbL3uautnX0s3ASXsTJQU5zCjNY0ZpAdNL8yjOz2Yw4ghHowxGHL2hQY73hGnrDdHWM8Dh1j6e2XeMDYeOMxh13hdfBg5ic9SeYGCAmdHRF35xcVFeFtOK82J95N422nvDL+7lZGYY04rz6OgL0z3wj+M3J+RnZ3L2lALmTyvinKmxW2lBNv3hCAPhKP2DEXIyM5hRlk91WQEl+f/cbTMYiXLgWA/bGzvZ3tjJ7qNdNHUOcKx7gNaeEJGoIz87k4XTi1kyo4RF04s5v2ZyQg9kkPSjIJAJpXtgkL/va+W5g20MDEZf3NMwDId7cW/DOUd5YS4LqopZML2Y6SV5/9RNF406DrX1sqOxk52NnRxu66W0IIfK4lwqCnOpKMp98ciqyQU55Of4O4gajTo6+sIUe3taIomiIBARSXOjDQL1dYiIpDkFgYhImlMQiIikOQWBiEiaUxCIiKQ5BYGISJpTEIiIpDkFgYhImpsQJ5SZWQtw6KTFJUDHaZad6vGJ+0OXlQPHxljmcPWcyToTsT2nWi/Z2nOqWkezTrzaM/S+2jO6Wkezzmjac/KyZG7PSM+d6WdytnOu4rSVOucm5A245XTLTvX4xP2TltXFs54zWWcitudU6yVbe8b7GcWrPSe1Te1JYHtG04Zkac9YPqOR7o/mNpG7hh4cxbJTPX5whHXiWc+ZrDMR23Oq9ZKtPaN9L7/bM9o6RkPtOfXy0y1L5vaM9Jwvn8mE6BpKFDOrc6OYl2OiUHuSm9qT3FKtPacykfcI/HBL0AXEmdqT3NSe5JZq7RmR9ghERNKc9ghERNJcSgaBmf3czJrNbNsYXnuemW01s71m9iMbctUTM/uome0ys+fN7NvxrfqUNcW9PWb2ZTNrMLNN3u2q+Fd+yrp8+Yy85z9jZs7MyuNX8Wlr8uMz+pqZbfE+n0fNbHr8Kx+xJj/a8x0z2+m16bdmVhr/ykesyY/2vNn7Loia2cQeSxjP4VHJegNeAawEto3hteuAi4hdGfGPwJXe8lcBfwZyvceVE7w9XwY+k0qfkffcTOARYuedlE/k9gDFQ9b5GPBfE7w9lwNZ3v1vAd+OtPN7AAAFwklEQVSa4O1ZAMwHngBqE9UWP24puUfgnHsKaBu6zMzmmNmfzGy9mf3VzM49+XVmVkXsP9+zLvZJ/wJ4g/f0h4FvOucGvG00+9uKl/jUnkD52Kabgc/xj1c89p0f7XHOdQ5ZdRIJbJNP7XnUOXfiwtF/B6r9bcVLfGrPDufcrkTU77eUDIIR3AJ81Dl3HvAZ4CfDrDMDqB/yuN5bBnAOcImZrTWzJ83sfF+rPb3xtgfgJm83/edmVuZfqaM2rjaZ2euABufcZr8LHaVxf0Zm9nUzOwK8Hfg3H2sdjXj8zp3wHmJ/XQcpnu2Z0LKCLiARzKwQeBlwz5Du5NzhVh1m2Ym/wrKAMuBC4HzgbjOb7f2VkFBxas9Pga95j78GfI/Yf85AjLdNZlYAfIlY90Pg4vQZ4Zz7EvAlM/sicBPw73EudVTi1R7vvb4EDAJ3xrPGMxHP9qSCtAgCYns+7c655UMXmlkmsN57+HtiX45Dd1ergRe8+/XA/d4X/zozixKbi6TFz8JHMO72OOeahrzuf4CH/Cx4FMbbpjnALGCz9x+7GthgZqucc0d9rn048fidG+ou4A8EFATEqT1mdiNwDXBZEH9EDRHvz2diC3qQwq8bUMOQgSHgb8CbvfsGLBvhdc8R+6v/xMDQVd7yDwFf9e6fAxzBOw9jgranasg6nwR+PdE/o5PWOUgCB4t9+ozmDVnno8C9E7w9VwDbgYpE/675+ftGCgwWB16ATx/4r4BGIEzsL/n3Evtr8U/AZu+X8d9GeG0tsA3YB/z4xJc9kAP80ntuA7B6grfn/wFbgS3E/vKpSlR7/GrTSeskNAh8+ozu85ZvITZ3zIwJ3p69xP6A2uTdEnkUlB/tuc57rwGgCXgkUe2J901nFouIpLl0OmpIRESGoSAQEUlzCgIRkTSnIBARSXMKAhGRNKcgkAnJzLoTvL2fmdnCOL1XxJtRdJuZPXi6WTjNrNTM/lc8ti0yHB0+KhOSmXU75wrj+H5Z7qUJ0Xw1tHYzuwPY7Zz7+inWrwEecs4tTkR9kn60RyApw8wqzOw+M3vOu13sLV9lZn8zs43ev/O95e8ys3vM7EHgUTO71MyeMLN7vXnz7xwy9/wTJ+acN7NubzK4zWb2dzOb6i2f4z1+zsy+Osq9lmd5adK8QjN73Mw2WGz++9d763wTmOPtRXzHW/ez3na2mNlX4vhjlDSkIJBU8kPgZufc+cD1wM+85TuBVzjnVhCbwfM/hrzmIuBG59xq7/EK4BPAQmA2cPEw25kE/N05twx4Cnj/kO3/0Nv+aeej8ea1uYzYmd0A/cB1zrmVxK5/8T0viL4A7HPOLXfOfdbMLgfmAauA5cB5ZvaK021PZCTpMumcpIdXAwuHzCZZbGZFQAlwh5nNIzZzZPaQ1zzmnBs6T/0651w9gJltIjY/zdMnbSfES5P0rQde492/iJeujXAX8N0R6swf8t7rgce85Qb8h/elHiW2pzB1mNdf7t02eo8LiQXDUyNsT+SUFASSSjKAi5xzfUMXmtl/Amucc9d5/e1PDHm656T3GBhyP8Lw/0fC7qXBtZHWOZU+59xyMyshFigfAX5E7JoDFcB5zrmwmR0E8oZ5vQHfcM799xluV2RY6hqSVPIosTn7ATCzE1MMlwAN3v13+bj9vxPrkgK44XQrO+c6iF2C8jNmlk2szmYvBF4FnO2t2gUUDXnpI8B7vDn1MbMZZlYZpzZIGlIQyERVYGb1Q26fIvalWusNoG4nNnU4wLeBb5jZM0CmjzV9AviUma0DqoCO073AObeR2OyXNxC7UEutmdUR2zvY6a3TCjzjHW76Hefco8S6np41s63AvfxjUIicER0+KhIn3lXS+pxzzsxuAN7mnHv96V4nEjSNEYjEz3nAj70jfdoJ8NKfImdCewQiImlOYwQiImlOQSAikuYUBCIiaU5BICKS5hQEIiJpTkEgIpLm/j97Un3LhWazMwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_dl = DataloaderCallback(learn, schedule_pars=(1.0, 0.05, 0, 10))\n",
    "cbs = [cb_dl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2' class='' max='4', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      50.00% [2/4 08:04<08:04]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th><lambda></th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>5.219692</th>\n",
       "    <th>5.067656</th>\n",
       "    <th>0.432184</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>4.720425</th>\n",
       "    <th>4.992826</th>\n",
       "    <th>0.437154</th>\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='60' class='' max='1309', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      4.58% [60/1309 00:07<02:30 4.7761]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of epoch, math 3210\n",
      "end of epoch, unmatch 3933\n",
      "15697 15697 15697 15697\n",
      "end of epoch, math 10617\n",
      "end of epoch, unmatch 4313\n",
      "15697 15697 15697 15697\n",
      "end of epoch, math 4158\n",
      "end of epoch, unmatch 10864\n",
      "15697 15697 15697 15697\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(4 , 1e-3, callbacks=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 3e-4\n",
    "lrs = [max_lr/100, max_lr/10, max_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage2_unfz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'{name}-stage2_unfz');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_dl = DataloaderCallback(learn, schedule_pars=(0.05, 0.01, 0, 10))\n",
    "cbs = [cb_dl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage3_unfz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage4_unfz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage5_unfz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage6_unfz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage7_unfz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage8_unfz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage9_unfz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage10_unfz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'{name}-stage3_unfz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_whale_fns = set(df[df['Id']=='new_whale'].sample(frac = 1).Image.iloc[:1000])\n",
    "#new_whale_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_fns = set(df[df.sighting_count == 2].Image)\n",
    "print(len(val_fns) + len(new_whale_fns))\n",
    "\n",
    "classes = df.Id.unique()\n",
    "\n",
    "df = df.drop(columns = ['sighting_count'])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = (\n",
    "    ImageItemList\n",
    "        .from_df(df, train_path, cols=['Image'])\n",
    "        .split_by_valid_func(lambda path: path2fn(path) in val_fns) ##.union(new_whale_fns)\n",
    "        .label_from_func(lambda path: fn2label[path2fn(path)], classes=classes)\n",
    "        .add_test(ImageItemList.from_folder(test_path))\n",
    "        .transform(get_transforms(do_flip=False), size=SZ, resize_method=ResizeMethod.SQUISH)\n",
    "        .databunch(bs=BS, num_workers=NUM_WORKERS, path=root_path)\n",
    "        .normalize(imagenet_stats)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "targs = []\n",
    "feats = []\n",
    "learn.model.eval()\n",
    "with torch.no_grad():\n",
    "    for ims, ts in data.valid_dl:\n",
    "        feats.append(learn.model.process_features(learn.model.cnn(ims)).detach().cpu())  ##\n",
    "        targs.append(ts)\n",
    "\n",
    "    feats = torch.cat(feats)\n",
    "    print(feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sims = []\n",
    "learn.model.eval()\n",
    "with torch.no_grad():\n",
    "    for feat in feats:\n",
    "        x1 = feats#.copy()\n",
    "        x2 = feat.unsqueeze(0).repeat(2570 ,1)\n",
    "        d1 = learn.model.calculate_distance(x1 , x2)\n",
    "        d2 = (x1 + x2)\n",
    "        d3 = (x1*x2)\n",
    "        d4 = (x1-x2)*(x1 - x2)\n",
    "        concat_layer = torch.cat([d1 ,d2,d3, d4]  ,dim = 1)\n",
    "        concat_layer = concat_layer.view( - 1, 1, num_features_model(learn.model.cnn) , 4)   ## no of channels is second dimension\n",
    "        concat_layer  = F.relu(learn.model.conv1(concat_layer.cuda()))\n",
    "        concat_layer = concat_layer.view(-1 ,1,32, num_features_model(learn.model.cnn)  )\n",
    "        concat_layer = F.relu(learn.model.conv2(concat_layer))\n",
    "        concat_layer_fn = concat_layer.view(-1 ,num_features_model(learn.model.cnn) )\n",
    "        #out = learn.model.head(concat_layer_fn)\n",
    "        predicted_similarity = learn.model.head(concat_layer_fn).sigmoid_()  #.cuda()\n",
    "        sims.append(predicted_similarity.squeeze().detach().cpu())\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sims[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_whale_idx = np.where(classes == 'new_whale')[0][0]\n",
    "new_whale_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5s = []\n",
    "learn.model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, sim in enumerate(sims):\n",
    "        idxs = sim.argsort(descending=False)\n",
    "        probs = sim[idxs]\n",
    "        top_5 = []\n",
    "        for j, p in zip(idxs, probs):\n",
    "            if len(top_5) == 5: break\n",
    "            if j == i: continue   \n",
    "            predicted_class = data.valid_ds.y.items[j]\n",
    "            \"\"\"\n",
    "            we dont want to predict new whale for validation data \n",
    "            \"\"\"\n",
    "            if predicted_class == new_whale_idx: continue\n",
    "            if predicted_class not in top_5: top_5.append(predicted_class)\n",
    "        top_5s.append(top_5)\n",
    "\n",
    "    ## top 5 contains 5 best predicted classes ,w ith indices from classes dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5s[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mapk of validation data set without having new whales in predictions. \n",
    "\"\"\"\n",
    "mapk(data.valid_ds.y.items.reshape(-1,1), np.stack(top_5s), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\"\"\"\n",
    "trying to calcualte threshold probability for new whale, which maximises the mapk for validation data.\n",
    "\"\"\"\n",
    "learn.model.eval()\n",
    "with torch.no_grad():\n",
    "    for thresh in np.linspace(0.7, 1, 20):\n",
    "        top_5s = []\n",
    "        for i, sim in enumerate(sims):\n",
    "            idxs = sim.argsort(descending=False)\n",
    "            probs = sim[idxs]\n",
    "            top_5 = []\n",
    "            for j, p in zip(idxs, probs):\n",
    "                if new_whale_idx not in top_5 and p < thresh and len(top_5) < 5: top_5.append(new_whale_idx)\n",
    "                if len(top_5) == 5: break\n",
    "                if j == i: continue\n",
    "                predicted_class = data.valid_ds.y.items[j]\n",
    "                if predicted_class not in top_5: top_5.append(predicted_class)\n",
    "            top_5s.append(top_5)\n",
    "        print(thresh, mapk(data.valid_ds.y.items.reshape(-1,1), np.stack(top_5s), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    ImageItemList\n",
    "        .from_df(df, train_path, cols=['Image'])\n",
    "        .split_by_valid_func(lambda path: path2fn(path) in {'69823499d.jpg'}) # in newer version of the fastai library there is .no_split that could be used here\n",
    "        .label_from_func(lambda path: fn2label[path2fn(path)], classes=classes)\n",
    "        .add_test(ImageItemList.from_folder(test_path))\n",
    "        .transform(None, size=SZ, resize_method=ResizeMethod.SQUISH)\n",
    "        .databunch(bs=BS, num_workers=NUM_WORKERS, path=root_path)\n",
    "        .normalize(imagenet_stats)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_feats = []\n",
    "learn.model.eval()\n",
    "with torch.no_grad():\n",
    "    for ims, _ in data.test_dl:\n",
    "        test_feats.append(learn.model.process_features(learn.model.cnn(ims)).detach().cpu())\n",
    "\n",
    "\n",
    "    test_feats = torch.cat(test_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_feats = []\n",
    "train_class_idxs = []\n",
    "learn.model.eval()\n",
    "with torch.no_grad():\n",
    "    for ims, t in data.train_dl:\n",
    "        train_feats.append(learn.model.process_features(learn.model.cnn(ims)).detach().cpu())\n",
    "        train_class_idxs.append(t)\n",
    "\n",
    "    train_class_idxs = torch.cat(train_class_idxs)\n",
    "    train_feats = torch.cat(train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_class_idxs)\n",
    "len(train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feats[0].expand(len(train_feats), 2048).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats = train_feats.cuda()\n",
    "test_feats = test_feats.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.cuda.empty_cache()\n",
    "sims = []\n",
    "learn.model.eval()\n",
    "with torch.no_grad():\n",
    "    tmp_batch_size = 1000\n",
    "    for i, feat in enumerate(test_feats):\n",
    "        if i % 1000 == 0:\n",
    "            print(i, len(test_feats))\n",
    "        #dists = learn.model.calculate_distance(train_feats, feat.unsqueeze(0).repeat(len(train_feats), 1))\n",
    "        row_sim = []\n",
    "        for k in range(0, len(train_feats), tmp_batch_size):\n",
    "            x1 = train_feats[k:k+tmp_batch_size]\n",
    "            x2 = feat.expand(x1.shape)\n",
    "            d1 = learn.model.calculate_distance(x1 , x2)\n",
    "            d2 = (x1 + x2)\n",
    "            d3 = (x1*x2)\n",
    "            d4 = (x1-x2)*(x1 - x2)\n",
    "            concat_layer = torch.cat([d1 ,d2,d3, d4]  ,dim = 1)\n",
    "            concat_layer = concat_layer.view( - 1, 1, num_features_model(learn.model.cnn) , 4)   ## no of channels is second dimension\n",
    "            concat_layer  = F.relu(learn.model.conv1(concat_layer.cuda()))\n",
    "            concat_layer = concat_layer.view(-1 ,1,32, num_features_model(learn.model.cnn)  )\n",
    "            concat_layer = F.relu(learn.model.conv2(concat_layer))\n",
    "            concat_layer_fn = concat_layer.view(-1 ,num_features_model(learn.model.cnn) )\n",
    "            predicted_similarity = learn.model.head(concat_layer_fn).sigmoid_()  #.cuda()\n",
    "            row_sim.append(predicted_similarity)\n",
    "        row_sim = torch.cat(row_sim)\n",
    "        sims.append(row_sim.squeeze().detach().cpu())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thresh = 0.82 #0.95\n",
    "\n",
    "top_5s = []\n",
    "\n",
    "for sim in sims:\n",
    "    idxs = sim.argsort(descending = False)\n",
    "    probs = sim[idxs]\n",
    "    top_5 = []\n",
    "    \n",
    "    \n",
    "    for  i , p in zip(idxs , probs):\n",
    "        if new_whale_idx not in top_5 and p < thresh  and len(top_5) < 5:\n",
    "            top_5.append(new_whale_idx)\n",
    "        if len(top_5) ==5: break\n",
    "        #if i == new_whale_idx: continue\n",
    "        predicted_class = train_class_idxs[i]\n",
    "        #print(predicted_class)\n",
    "        if predicted_class == new_whale_idx: continue\n",
    "        if predicted_class not in top_5:\n",
    "            top_5.append(predicted_class)\n",
    "    top_5s.append(top_5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_5s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_classes  = []\n",
    "\n",
    "for top_5 in top_5s:\n",
    "    top_5_classes.append(' '.join([classes[t] for t in top_5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'Image': [path.name for path in data.test_ds.x.items]})\n",
    "sub['Id'] = top_5_classes\n",
    "sub.to_csv(f'../submission/{name}.csv', index=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f'../submission/{name}.csv').Id.str.split().apply(lambda x: x[0] == 'new_whale').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name = 'Ensembleing_resnet50_renet101_siamene_v1'\n",
    "! kaggle competitions submit -c humpback-whale-identification -f ../submission/martin_siamene_network_15k_training_images.csv -m \"resnet18 arch prob 1 for new whales\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
