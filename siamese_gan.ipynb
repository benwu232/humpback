{
  "cells": [
    {
      "metadata": {
        "_uuid": "66456b04405182f1e5583058307f702b130fb40e"
      },
      "cell_type": "markdown",
      "source": "## This is my attempt to use siamese with gan idea"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "906946c5539307d4d006ac922f6516c8ca73973d",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "use_kaggle = 1\n\nif use_kaggle:\n    #!pip install git+https://github.com/fastai/fastai.git\n    !git clone https://github.com/benwu232/humpback\n    import sys\n     # Add directory holding utility functions to path to allow importing utility funcitons\n    #sys.path.insert(0, '/kaggle/working/protein-atlas-fastai')\n    sys.path.append('/kaggle/humback/')",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Cloning into 'humpback'...\nremote: Enumerating objects: 103, done.\u001b[K\nremote: Counting objects: 100% (103/103), done.\u001b[K\nremote: Compressing objects: 100% (69/69), done.\u001b[K\nremote: Total 264 (delta 65), reused 67 (delta 34), pack-reused 161\u001b[K\nReceiving objects: 100% (264/264), 19.05 MiB | 34.53 MiB/s, done.\nResolving deltas: 100% (164/164), done.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de5edc472d60a4712451c4697bcdac9491052942"
      },
      "cell_type": "code",
      "source": "# Suppress annoying stderr output when importing keras.\nimport sys\nimport platform\nold_stderr = sys.stderr\nsys.stderr = open('/dev/null' if platform.system() != 'Windows' else 'nul', 'w')\n\nsys.stderr = old_stderr\n\nimport random\nfrom scipy.ndimage import affine_transform\n\nimport pickle\nimport numpy as np\nfrom math import sqrt\n\n# Determise the size of each image\nfrom os.path import isfile\nfrom PIL import Image as pil_image\nfrom tqdm import tqdm_notebook\n\nfrom pandas import read_csv\nimport pandas as pd\nfrom pathlib import Path\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom fastai.vision import *\nfrom fastai.metrics import accuracy_thresh\nfrom fastai.basic_data import *\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch import nn\nfrom fastai.callbacks.hooks import num_features_model, model_sizes\nfrom fastai.layers import BCEWithLogitsFlat\nfrom fastai.basic_train import Learner\nfrom skimage.util import montage\nimport pandas as pd\nfrom torch import optim\nimport re\n\nif use_kaggle:\n    from humpback.utils import *\nelse:\n    from utils import *\n    \nfrom IPython.core.debugger import set_trace\n#from functional import seq",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "93bb20436b38d8f58ad0d878e279c06ea6f0801e"
      },
      "cell_type": "code",
      "source": "fastai.__version__",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "'1.0.39'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2362a475b45c5d126eaf17106cdbfddec1bb3cb2"
      },
      "cell_type": "code",
      "source": "root_path = Path('../input')\ntrain_path = root_path/'train'\ntest_path = root_path/'test'\n\nUSE_CUDA = torch.cuda.is_available()\n\n#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice = torch.device(\"cuda\" if USE_CUDA else \"cpu\")",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5cb2990ac2fe2a4f733535b1c755afbef35b6f96"
      },
      "cell_type": "code",
      "source": "name = f'siamese_resnet18_224'\n\n#arch = models.resnet50\narch = models.resnet18\nim_size = 224\ntrain_batch_size = 64\nval_batch_size = 128\nif use_kaggle:\n    dl_workers = 0\nelse:\n    dl_workers = 6\nSEED=0\nemb_len = 256",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f6d315086f994303b0f4d67b4c968621eb437a29"
      },
      "cell_type": "code",
      "source": "df0 = pd.read_csv('../input/train.csv')\ndf_new = df0[df0.Id == 'new_whale']\ndf_known = df0[df0.Id != 'new_whale']\ntrain_list, val_list = split_whale_set(df0, nth_fold=0, new_whale_method=1, seed=1)\n\nim_count = df0[df0.Id != 'new_whale'].Id.value_counts()\nim_count.name = 'sighting_count'\nex_df = df0.join(im_count, on='Id')\n\npath2fn = lambda path: re.search('\\w*\\.jpg$', path).group(0)\nfn2label = {row[1].Image: row[1].Id for row in df0.iterrows()}\nclass_dict = make_whale_class_dict(df0)\nfile_lut = df0.set_index('Image').to_dict()",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "12ef302f15102e4b78bda45359e95e9f0ff61a68"
      },
      "cell_type": "code",
      "source": "len(val_list)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "5471"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "d203f911cec27d8decb301936c2da0b5e1f09252"
      },
      "cell_type": "code",
      "source": "im_tfms = get_transforms(do_flip=False, max_zoom=1, max_warp=0, max_rotate=2)\n\ndata = (\n    ImageItemList\n        # .from_df(df_known, 'data/train', cols=['Image'])\n        .from_folder(train_path)\n        # .split_by_idxs(train_item_list, val_item_list)\n        .split_by_valid_func(lambda path: path2fn(str(path)) in val_list)\n        # .split_by_idx(val_list)\n        # .random_split_by_pct(seed=SEED)\n        .label_from_func(lambda path: fn2label[path2fn(str(path))])\n        #.add_test(ImageItemList.from_folder(test_path))\n        #.transform([None, None], size=im_size, resize_method=ResizeMethod.SQUISH)\n        .transform(im_tfms, size=im_size, resize_method=ResizeMethod.SQUISH)\n        .databunch(bs=train_batch_size, num_workers=dl_workers, path=root_path)\n        #.normalize(imagenet_stats)\n)\n\ndata.add_tfm(normalize_batch)\n\ntrain_dl = DataLoader(\n    SiameseDs(data.train_ds, data.train_dl),\n    batch_size=train_batch_size,\n    shuffle=True,\n    collate_fn=collate_siamese,\n    num_workers=dl_workers\n)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed8a09f69418db00aa1ba2daa3d8986d9a242598"
      },
      "cell_type": "code",
      "source": "data_v = (\n    ImageItemList\n        # .from_df(df_known, 'data/train', cols=['Image'])\n        .from_folder(train_path)\n        # .split_by_idxs(train_item_list, val_item_list)\n        .split_by_valid_func(lambda path: path2fn(str(path)) in val_list)\n        # .split_by_idx(val_list)\n        # .random_split_by_pct(seed=SEED)\n        .label_from_func(lambda path: fn2label[path2fn(str(path))])\n        .add_test(ImageItemList.from_folder(test_path))\n        #.transform([None, None], size=im_size, resize_method=ResizeMethod.SQUISH)\n        #.transform(im_tfms, size=im_size, resize_method=ResizeMethod.SQUISH)\n        .databunch(bs=train_batch_size, num_workers=dl_workers, path=root_path)\n        .normalize(imagenet_stats)\n)\n\n#v = SimpleDataset(data.valid)\nvalid_dl = DataLoader(\n    SimpleDataset(data_v.valid_ds),\n    batch_size=val_batch_size,\n    shuffle=False,\n    drop_last=False,\n    num_workers=dl_workers\n)\n\ntest_dl = DataLoader(\n    SimpleDataset(data_v.test_ds),\n    batch_size=val_batch_size,\n    shuffle=False,\n    drop_last=False,\n    num_workers=dl_workers\n)\n\ndata_ref = (\n    ImageItemList\n        .from_df(df_known, train_path, cols=['Image'])\n        #.from_folder(train_path)\n        # .split_by_idxs(train_item_list, val_item_list)\n        .split_by_valid_func(lambda path: path2fn(str(path)) in val_list)\n        # .split_by_idx(val_list)\n        # .random_split_by_pct(seed=SEED)\n        .label_from_func(lambda path: fn2label[path2fn(str(path))])\n        #.add_test(ImageItemList.from_folder(test_path))\n        #.transform([None, None], size=im_size, resize_method=ResizeMethod.SQUISH)\n        #.transform(im_tfms, size=im_size, resize_method=ResizeMethod.SQUISH)\n        .databunch(bs=val_batch_size, num_workers=dl_workers, path=root_path)\n        .normalize(imagenet_stats)\n)\nprint(len(data_ref.train_ds), len(data_ref.valid_ds))\n\nref_dl = DataLoader(\n        SimpleDataset(data_ref.train_ds),\n        batch_size=val_batch_size,\n        shuffle=False,\n        #collate_fn=siamese_collate,\n        num_workers=dl_workers\n    )",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/fastai/basic_data.py:215: UserWarning: It's not possible to collate samples of your dataset together in a batch.\nShapes of the inputs/targets:\n[[torch.Size([3, 615, 1076]), torch.Size([3, 811, 1050]), torch.Size([3, 450, 1050]), torch.Size([3, 278, 1050]), torch.Size([3, 344, 1050]), torch.Size([3, 750, 1050]), torch.Size([3, 525, 1050]), torch.Size([3, 318, 1050]), torch.Size([3, 321, 1050]), torch.Size([3, 354, 1050]), torch.Size([3, 450, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 726, 1050]), torch.Size([3, 450, 1050]), torch.Size([3, 603, 1050]), torch.Size([3, 652, 1000]), torch.Size([3, 700, 1050]), torch.Size([3, 450, 1050]), torch.Size([3, 702, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 544, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 419, 1050]), torch.Size([3, 322, 506]), torch.Size([3, 342, 1050]), torch.Size([3, 828, 1050]), torch.Size([3, 450, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 334, 1050]), torch.Size([3, 476, 686]), torch.Size([3, 543, 1050]), torch.Size([3, 675, 1050]), torch.Size([3, 500, 700]), torch.Size([3, 600, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 786, 1050]), torch.Size([3, 630, 1050]), torch.Size([3, 242, 1050]), torch.Size([3, 263, 1050]), torch.Size([3, 314, 669]), torch.Size([3, 759, 1050]), torch.Size([3, 450, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 301, 1050]), torch.Size([3, 318, 1050]), torch.Size([3, 450, 1050]), torch.Size([3, 747, 2150]), torch.Size([3, 525, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 652, 1000]), torch.Size([3, 600, 1050]), torch.Size([3, 602, 1047]), torch.Size([3, 616, 1050]), torch.Size([3, 500, 699]), torch.Size([3, 408, 1050]), torch.Size([3, 299, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 525, 1050]), torch.Size([3, 269, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 700, 1050])], [(), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), ()]]\n  warn(message)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "You can deactivate this warning by passing `no_check=True`.\nYou can deactivate this warning by passing `no_check=True`.\n12158 3539\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/fastai/basic_data.py:215: UserWarning: It's not possible to collate samples of your dataset together in a batch.\nShapes of the inputs/targets:\n[[torch.Size([3, 700, 1050]), torch.Size([3, 695, 5959]), torch.Size([3, 401, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 465, 1050]), torch.Size([3, 521, 1050]), torch.Size([3, 292, 682]), torch.Size([3, 432, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 206, 699]), torch.Size([3, 512, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 450, 1050]), torch.Size([3, 609, 1050]), torch.Size([3, 525, 1050]), torch.Size([3, 390, 1050]), torch.Size([3, 367, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 248, 1050]), torch.Size([3, 503, 700]), torch.Size([3, 343, 1050]), torch.Size([3, 313, 1050]), torch.Size([3, 333, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 242, 702]), torch.Size([3, 599, 1050]), torch.Size([3, 451, 1050]), torch.Size([3, 450, 1050]), torch.Size([3, 591, 1050]), torch.Size([3, 751, 1050]), torch.Size([3, 656, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 332, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 263, 1050]), torch.Size([3, 650, 1050]), torch.Size([3, 675, 1050]), torch.Size([3, 700, 1800]), torch.Size([3, 358, 1050]), torch.Size([3, 488, 1050]), torch.Size([3, 320, 1050]), torch.Size([3, 211, 745]), torch.Size([3, 750, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 525, 1050]), torch.Size([3, 525, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 600, 920]), torch.Size([3, 748, 1008]), torch.Size([3, 450, 1050]), torch.Size([3, 344, 1050]), torch.Size([3, 469, 938]), torch.Size([3, 600, 900]), torch.Size([3, 243, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 125, 545]), torch.Size([3, 330, 1134]), torch.Size([3, 347, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 338, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 272, 476]), torch.Size([3, 420, 894]), torch.Size([3, 700, 1050]), torch.Size([3, 600, 900]), torch.Size([3, 314, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 529, 700]), torch.Size([3, 309, 1050]), torch.Size([3, 117, 199]), torch.Size([3, 414, 1050]), torch.Size([3, 591, 1050]), torch.Size([3, 225, 450]), torch.Size([3, 525, 1050]), torch.Size([3, 450, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 369, 553]), torch.Size([3, 700, 1050]), torch.Size([3, 273, 1397]), torch.Size([3, 700, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 192, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 652, 1000]), torch.Size([3, 450, 1050]), torch.Size([3, 241, 1050]), torch.Size([3, 701, 1050]), torch.Size([3, 624, 1050]), torch.Size([3, 716, 856]), torch.Size([3, 525, 1050]), torch.Size([3, 688, 1050]), torch.Size([3, 601, 1050]), torch.Size([3, 340, 1046]), torch.Size([3, 597, 1050]), torch.Size([3, 591, 1050]), torch.Size([3, 345, 805]), torch.Size([3, 987, 1010]), torch.Size([3, 194, 552]), torch.Size([3, 600, 1050]), torch.Size([3, 268, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 363, 1050]), torch.Size([3, 450, 1050]), torch.Size([3, 400, 957]), torch.Size([3, 600, 1050]), torch.Size([3, 700, 1800]), torch.Size([3, 192, 800]), torch.Size([3, 450, 1050]), torch.Size([3, 316, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 477, 835]), torch.Size([3, 287, 1050]), torch.Size([3, 213, 1050]), torch.Size([3, 448, 1049]), torch.Size([3, 525, 1050]), torch.Size([3, 450, 1050]), torch.Size([3, 701, 1050])], [(), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), ()]]\n  warn(message)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3f8f2d153e9970300574e135c523e1e43b740259"
      },
      "cell_type": "code",
      "source": "data_bunch = ImageDataBunch(train_dl, valid_dl, fix_dl=ref_dl, device=device)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "168c45a50e0ea160109e8bc6c2bcb21c8ddc8fd6"
      },
      "cell_type": "code",
      "source": "siamese = SiameseNetwork(arch=arch)\nsiamese.to(device)",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /tmp/.torch/models/resnet18-5c106cde.pth\n100%|██████████| 46827520/46827520 [00:01<00:00, 26916254.95it/s]\n",
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "SiameseNetwork(\n  (cnn): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (fc): Linear(in_features=512, out_features=1, bias=True)\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "631ad12f3ed69a96e12d4d80ea171e372ad18eb1"
      },
      "cell_type": "code",
      "source": "learn = LearnerEx(data_bunch,\n                  siamese,\n                  enable_validate=False,\n                  loss_func=BCEWithLogitsFlat(),\n                  #loss_func=contrastive_loss,\n                  #metrics=[lambda preds, targs: accuracy_thresh(preds.squeeze(), targs, sigmoid=False)]\n                  )",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b04638dc94d032ab52162cc29d44e0bc55ae9530"
      },
      "cell_type": "code",
      "source": "learn.split([learn.model.cnn[:6], learn.model.cnn[6:], learn.model.fc])",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3db56b21206b303f5ef78632597d61ab9f21280b"
      },
      "cell_type": "code",
      "source": "from fastai.callbacks import SaveModelCallback\ncb_save_model = SaveModelCallback(learn, every=\"epoch\", name=f\"siamese\")\n#cb_siamese_validate = SiameseValidateCallback(learn, txlog)\ncbs = [cb_save_model]#, cb_siamese_validate]",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "596b9aad355f1fce6be5989a717b71c5fab75b8f"
      },
      "cell_type": "code",
      "source": "learn.freeze_to(-1)\nlearn.fit_one_cycle(1)\nlearn.unfreeze()",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n      0.00% [0/1 00:00<00:00]\n    </div>\n    \n<table style='width:300px; margin-bottom:10px'>\n  <tr>\n    <th>epoch</th>\n    <th>train_loss</th>\n    <th>valid_loss</th>\n  </tr>\n</table>\n\n\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='311', style='width:300px; height:20px; vertical-align: middle;'></progress>\n      0.00% [0/311 00:00<00:00]\n    </div>\n    "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1cf65ceea3c1d43f351b2392e755a154774e24a1"
      },
      "cell_type": "code",
      "source": "enable_lr_find = 1\nif enable_lr_find:\n    print('LR plotting ...')\n    learn.lr_find()\n    learn.recorder.plot()\n    plt.savefig('lr_find.png')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ac59fa95808d328cc7574d6be08cd745042a8799"
      },
      "cell_type": "code",
      "source": "max_lr = 5e-4\nlrs = [max_lr/100, max_lr/10, max_lr]\nlearn.fit_one_cycle(100, lrs, callbacks=cbs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "21e0ae2a6cc37dd5a53215cde165949eac5ec165"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bd33bbeb2f61393ba0fa162e5c95359cd88e3aed"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b8151ae994dc88eee113caf9d445ba29f28bb88"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e0a4979ade8506e4f44eac86ac8fc81c2d79e22c"
      },
      "cell_type": "markdown",
      "source": "For training data, we are taking all the images except for the category - New Whale"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a58ebdfaff7509d2d13a168e5d98637c7a79ecd5"
      },
      "cell_type": "code",
      "source": "new_df = df[df['Id']!= 'new_whale']\n#new_df = new_df[new_df['sighting_count']>1]\n\nprint('shape of data for training ',new_df.shape)\nnew_df.drop(columns = ['sighting_count'] , inplace=True)\nnew_df.head(2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db741211d7ac577fc34451c2e3bd61d3084668e0"
      },
      "cell_type": "code",
      "source": "new_df.reset_index(inplace=True)\nnew_df.drop(columns='index' , inplace=True)\nnew_df.tail(3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9be02143d6b46c3cc2349e37eab4ee19c14d0c54"
      },
      "cell_type": "code",
      "source": "new_df.to_records()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "707d8411bc15534542795bca17922ac2c181f52e"
      },
      "cell_type": "code",
      "source": "\"\"\"\nThe data set we are using for training contains all images except of new whales.\nwe don't require creating phase values for this  datasets , as duplicate images are very few.\ni am using index present in train.csv as the phase value as we can use it for indexing very easily \n\"\"\"\n\ntagged = dict([(p,w) for _,p,w in new_df.to_records()])\nh2ps = dict([(idx , p ) for   idx,p,w in new_df.to_records()])\np2h   = dict([(p , idx) for idx , p , w in new_df.to_records()])\nh2p = h2ps.copy()\njoin = tagged.copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "54daa4f1950bba3f42133287b7f530ebad9e5824"
      },
      "cell_type": "code",
      "source": "def expand_path(p):\n    if isfile(train_path/p): \n        return train_path/p\n    if isfile(test_path/p): \n        return test_path/p\n    return p\n\np2size = {}\nfor p in tqdm_notebook(join):\n    size      = pil_image.open(expand_path(p)).size\n    p2size[p] = size\nlen(p2size), list(p2size.items())[:5]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "98e3ec25fcf68f22ca706ded4db56e3948263a1e"
      },
      "cell_type": "code",
      "source": "## phase value for all categories except new whale\nh2ws = {}\nnew_whale = 'new_whale'\nfor p,w in tagged.items():\n    if w != new_whale: # Use only identified whales\n        h = p2h[p]\n        if h not in h2ws: h2ws[h] = []\n        if w not in h2ws[h]: h2ws[h].append(w)\nfor h,ws in h2ws.items():\n    if len(ws) > 1:\n        h2ws[h] = sorted(ws)\nlen(h2ws)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c411335bacdbbc578c2085e96a282dddaf31322"
      },
      "cell_type": "code",
      "source": "## for each whale category, observe the associated phase values , \n##store all whale categories even the categories with just one image ( this is  different from martin's approach)\n\nw2hs = {}\nfor h,ws in h2ws.items():\n    if len(ws) == 1: # Use only unambiguous pictures\n\n        w = ws[0]\n        if w not in w2hs: w2hs[w] = []\n        if h not in w2hs[w]: w2hs[w].append(h)\nfor w,hs in w2hs.items():\n    #if len(hs) > 1:\n        w2hs[w] = sorted(hs)\nlen(w2hs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b570d288a34784230903e7245bc46973dfa2f1d"
      },
      "cell_type": "code",
      "source": "len(h2ws)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "21586ee1f6f5b964cce7c0f8fbc79f11c89d8f6d"
      },
      "cell_type": "code",
      "source": "def read_raw_image(p):\n    img = pil_image.open(expand_path(p))\n    return img\n\n\nimport matplotlib.pyplot as plt\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "583584e94ed126ccb4835c5b55431aee42236006"
      },
      "cell_type": "code",
      "source": "train = [] # A list of  indices of images to be used in training data.\nfor hs in w2hs.values():\n    if len(hs) >= 1:\n        train += hs\nrandom.shuffle(train)\ntrain_set = set(train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f60c7328c1334c5f3aebb1f0643a82be1b92340"
      },
      "cell_type": "code",
      "source": "## we have whales categories with phases(images) more than 1. shuffle the phase values now.\nw2ts = {} #Associate the image index from train to each whale id.\nfor w,hs in w2hs.items():\n    for h in hs:\n        if h in train_set:\n            if w not in w2ts: w2ts[w] = []\n            if h not in w2ts[w]: w2ts[w].append(h)\nfor w,ts in w2ts.items(): w2ts[w] = np.array(ts)\n## then again for each whale categories see how many images you have , \n## you are working with 5004 whale categories and 15697 images \n    \n    \nt2i = {} # The position in train of each training image id\nfor i,t in enumerate(train): t2i[t] = i\n\nlen(train),len(w2ts)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "80ad7b4a9c36babb90d16d35bf1a3e2c32e6c592"
      },
      "cell_type": "code",
      "source": "# from keras.utils import Sequence\n# import keras\n\nfrom IPython.core.debugger import set_trace\nimport random\n#from keras import backend as K\n\ntry:\n    from lapjv import lapjv\n    segment = False\nexcept ImportError:\n    print('Module lap not found, emulating with much slower scipy.optimize.linear_sum_assignment')\n    segment = True\n    from scipy.optimize import linear_sum_assignment\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bf012efe99894b7b3105b1212a648ce6e616e26c"
      },
      "cell_type": "markdown",
      "source": "Import functions from fast ai library"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "21b6f35a9217d969c35e6cbe6deadd2a91273611"
      },
      "cell_type": "code",
      "source": "\nfn2label = {row[1].Image: row[1].Id for row in df.iterrows()}  #new_\npath2fn = lambda path: re.search('\\w*\\.jpg$', path).group(0)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4c181a8e94c8562c95b912b83b6d15441e1a1450"
      },
      "cell_type": "markdown",
      "source": "Creating dataset for all the training images. Because of some reason , i am not able to create validation set as well ( produces error while indexing from match and unmatch matrices. If someone is able to find the work arounf the help will be appreciated"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d8ee9b67e7f566f5a6171bc62fce7336f47d1ce8"
      },
      "cell_type": "code",
      "source": "classes = df.Id.unique()\ndata = (\n    ImageItemList  ##df[(df.Id != 'new_whale') & (df.sighting_count >1)]\n        .from_df( df[(df.Id != 'new_whale')], train_path, cols=['Image'])\n        .no_split()##split_by_valid_func(lambda path: path2fn(path) in val_fns) \n        .label_from_func(lambda path: fn2label[path2fn(path)] ,  classes=classes)\n        .add_test(ImageItemList.from_folder(test_path))\n        .transform(get_transforms(do_flip=False), size=SZ, resize_method=ResizeMethod.SQUISH))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "78f6fa8a1799126b9d77278bfef8fb296fe13365"
      },
      "cell_type": "code",
      "source": "print(len(train))\nprint(len(data.train.x))\nprint(len(data.valid.x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "022ea0dcfd8c06c719818784bc1e650006d35864"
      },
      "cell_type": "code",
      "source": "from IPython.core.debugger import set_trace\nimport random\n\n# First try to use lapjv Linear Assignment Problem solver as it is much faster.\n# At the time I am writing this, kaggle kernel with custom package fail to commit.\n# scipy can be used as a fallback, but it is too slow to run this kernel under the time limit\n# As a workaround, use scipy with data partitioning.\n# Because algorithm is O(n^3), small partitions are much faster, but not what produced the submitted solution\ntry:\n    from lapjv import lapjv\n    segment = False\nexcept ImportError:\n    print('Module lap not found, emulating with much slower scipy.optimize.linear_sum_assignment')\n    segment = True\n    from scipy.optimize import linear_sum_assignment\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0ac7970666823565a0e4d61ea962d6800c1a2f13"
      },
      "cell_type": "markdown",
      "source": "TwoImDataset creation is the part where I am trying to replicate 'TrainingData Class' from https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563/output\nFor whale categories having just one images in training data , matching pair -  same image pair (A,A) . For other categories it creates a de arrangement.  "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "192c2491b34cd71b2dcdebf2a6a1114799e81c65"
      },
      "cell_type": "code",
      "source": "np.random.random()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cf28a71941aa7b2904f1d29d0829897eafc979b0"
      },
      "cell_type": "code",
      "source": "import pdb\ndef is_even(num): return num % 2 == 0\n\nclass TwoImDataset(Dataset):\n    def __init__(self, ds, score, steps = 1000):\n        self.ds = ds\n        self.whale_ids = ds.y.items\n        self.steps =1000\n        self.score  = -score\n        for ts in w2ts.values():\n            idxs =  ts.copy() #[t2i[t] for t in ts]\n            #idxs = [i for i in  idxs if i <score.shape[0]]\n            for i in idxs:\n                for j in idxs:\n                    self.score[i,j] = 10000.0   # Set a large value for matching\n        self.epsilon = 1.0\n        self.on_epoch_end()\n        \n    def set_epsilon(self, epsilon=1.0):\n        self.epsilon = epsilon\n        \n    def __len__(self):\n        return 2 * len(self.ds)\n\n    def __getitem__(self, idx):\n        prob = np.random.random()\n        choice = idx % 2 == 0\n        if choice:\n            idx //=2\n        else:\n            idx = (idx - 1) // 2\n        \n        if prob < self.epsilon: \n            return self.sample_random(idx, choice)\n        else:\n            return self.sample_policy(idx, choice)\n                \n    def sample_random(self, idx, choice):\n        if choice:\n            return self.sample_same(idx)\n        else: return self.sample_different(idx)\n\n    def sample_same(self, idx):\n        whale_id = self.whale_ids[idx]        \n        candidates = list(np.where(self.whale_ids == whale_id)[0])\n        candidates.remove(idx) # dropping our current whale - we don't want to compare against an identical image!\n        \n        if len(candidates) == 0: # oops, there is only a single whale with this id in the dataset\n            return self.sample_different(idx)\n        \n        np.random.shuffle(candidates)\n        return self.construct_example(self.ds[idx][0], self.ds[candidates[0]][0], 1)\n    \n    def sample_different(self, idx):\n        whale_id = self.whale_ids[idx]\n        candidates = list(np.where(self.whale_ids != whale_id)[0])\n        np.random.shuffle(candidates)\n        return self.construct_example(self.ds[idx][0], self.ds[candidates[0]][0], 0)\n  \n    def sample_policy(self, idx , tag):\n        #set_trace()\n        if tag==0:\n            first_image_id =  self.match[idx][0]\n            second_image_id = self.match[idx][1]\n            #if first_image_id < len(self.ds) and second_image_id< len(self.ds):         \n            return self.construct_example(self.ds[first_image_id][0], self.ds[second_image_id][0], 1)\n        else:\n            first_image_id =  self.unmatch[idx][0]\n            second_image_id = self.unmatch[idx][1]     \n            return self.construct_example(self.ds[first_image_id][0], self.ds[second_image_id][0], 0)\n  \n    def on_epoch_end(self):\n        if self.steps <= 0: return # Skip this on the last epoch.\n        self.steps     -= 1\n        self.match      = []\n        self.unmatch    = []\n        if segment:\n            tmp   = []\n            batch = 512\n            for start in range(0, score.shape[0], batch):\n                end = min(score.shape[0], start + batch)\n                _, x = linear_sum_assignment(self.score[start:end, start:end])\n                tmp.append(x + start)\n            x = np.concatenate(tmp)\n        else:\n            #print('using lapjv')\n            x,_,_ = lapjv(self.score) # Solve the linear assignment problem\n        y = np.arange(len(x),dtype=np.int32)\n\n        # Compute a derangement for matching whales\n        for ts in w2ts.values():\n            d = ts.copy()\n            if (len(d)==1):\n                for ab in zip(ts,d): self.match.append(ab)\n            else:                \n                while True:\n                    random.shuffle(d)\n                    if not np.any(ts == d): break\n                for ab in zip(ts,d): self.match.append(ab)\n\n        # Construct unmatched whale pairs from the LAP solution.\n        for i,j in zip(x,y):\n            if i == j:\n                print(self.score)\n                print(x)\n                print(y)\n                print(i,j)\n            assert i != j\n            self.unmatch.append((train[i],train[j]))\n\n        # Force a different choice for an eventual next epoch.\n        self.score[x,y] = 10000.0\n        self.score[y,x] = 10000.0\n        random.shuffle(self.match)\n        random.shuffle(self.unmatch)\n        print('end of epoch, math',self.match[0][0])\n        print('end of epoch, unmatch',self.unmatch[0][0])\n        \n        print(len(self.match), len(train), len(self.unmatch), len(train))\n        #assert len(self.match) == len(train) and len(self.unmatch) == len(train)\n    \n    def construct_example(self, im_A, im_B, class_idx):\n        return [im_A, im_B], class_idx",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "c290bbc91b554d25d193cbbff88fea58cf2680da"
      },
      "cell_type": "code",
      "source": "\"\"\"\nCreate a 2D score matrix of size of training data\n\"\"\"\n\nscore = np.random.random_sample(size=(len(train),len(train)))\n\ntrain_dl = DataLoader(\n    TwoImDataset(data.train , score),\n    batch_size=BS,\n    shuffle=True,\n    num_workers=NUM_WORKERS\n)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9f92c507d697957a48fc20f2653f1ce582ce673"
      },
      "cell_type": "code",
      "source": "def normalize_batch(batch):\n    stat_tensors = [torch.tensor(l).cuda() for l in imagenet_stats]\n    return [normalize(batch[0][0], *stat_tensors), normalize(batch[0][1], *stat_tensors)], batch[1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72f642d102e722b052ace01b5ae3791aeb98ea8f"
      },
      "cell_type": "code",
      "source": "data_bunch = ImageDataBunch(train_dl , train_dl) ##, valid_dl\ndata_bunch.add_tfm(normalize_batch)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "91e7aa7456f6b4d7bd81a898e33670b924defca2"
      },
      "cell_type": "code",
      "source": "\"\"\"\nThe netowrk architecture is also inspired from Martin's notebook (part after we extract features for two image pairs)\n\"\"\"\nclass SiameseNetwork(nn.Module):\n    def __init__(self, arch=models.resnet50):\n        super().__init__() \n        \n        self.cnn = create_body(arch)\n        self.head = nn.Linear(num_features_model(self.cnn), 1)  #\n        \n        self.conv1 = nn.Conv2d(1 , 32 , kernel_size= (1 , 4) , padding = 0 ,stride=1)\n        self.conv2 = nn.Conv2d( 1 , 1 , kernel_size = (32 ,1 ) , padding = 0  , stride=1)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, im_A, im_B):\n        x1, x2 = seq(im_A, im_B).map(self.cnn).map(self.process_features)\n        d1 = self.calculate_distance(x1, x2)\n        d2 = (x1 + x2)\n        d3 = (x1*x2)\n        d4 = (x1-x2)*(x1 - x2)\n        concat_layer = torch.cat([d1 ,d2,d3, d4]  ,dim = 1)\n        concat_layer = concat_layer.view( - 1, 1, num_features_model(self.cnn) , 4)   ## no of channels is second dimension\n        concat_layer  = F.relu(self.conv1(concat_layer))\n        concat_layer = concat_layer.view(-1 ,1,32, num_features_model(self.cnn))\n        concat_layer = F.relu(self.conv2(concat_layer))\n        concat_layer_fn = concat_layer.view(-1 ,num_features_model(self.cnn))\n        dropt = self.dropout(concat_layer_fn)\n        out = self.head(dropt)\n        return out\n    \n    def process_features(self, x): \n        y = x.reshape(*x.shape[:2], -1)\n        return x.reshape(*x.shape[:2], -1).max(-1)[0]\n    def calculate_distance(self, x1, x2): return (x1 - x2).abs_()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95bddab99cbd5025e7cf798a4207b77d94658581"
      },
      "cell_type": "code",
      "source": "learn = Learner(data_bunch, SiameseNetwork(), \n                loss_func=BCEWithLogitsFlat(), \n                wd=0.01,\n                metrics=[lambda preds, targs: accuracy_thresh(preds.squeeze(), targs, sigmoid=False)])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0dec12a3603df5f2d08642641e7f2dd19cb9fb06"
      },
      "cell_type": "code",
      "source": "def linear_schedule(step, pars):\n    \"Linearly output value, end_step must greater than start_step\"\n    start_value = pars[0]\n    end_value = pars[1]\n    start_step = pars[2]\n    end_step = pars[3]\n    assert start_step <= end_step\n\n    if step < start_step:\n        return start_value\n    elif step >= end_step:\n        return end_value\n    return start_value - (step - start_step) * (start_value - end_value) / (end_step - start_step)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0fbe9f4bc37ea2d37e799db16548f26088b4529b"
      },
      "cell_type": "code",
      "source": "linear_decay = partial(linear_schedule, pars=(1.0, 0.05, 2, 12))\nlinear_decay(0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f464689e671c475f34633b9ff3db822f5f6e9e56"
      },
      "cell_type": "code",
      "source": "class DataloaderCallback(fastai.callbacks.tracker.TrackerCallback):\n    def __init__(self, learn, schedule_pars=(1.0, 0.05, 0, 10)):\n        super().__init__(learn)\n        self.schedule = partial(linear_schedule, pars=schedule_pars)\n        \n    #def on_batch_end(self, last_loss, epoch, num_batch, **kwargs: Any) -> None:\n    def on_epoch_begin(self, epoch, **kwargs: Any) -> None:\n        epsilon = self.schedule(epoch)\n        self.learn.data.train_ds.set_epsilon(epsilon)\n        self.learn.data.train_ds.on_epoch_end()\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e40e29e94dac856490f179f7637080e0c9bc5365"
      },
      "cell_type": "code",
      "source": "learn.split([learn.model.cnn[:6], learn.model.cnn[6:], learn.model.head])\nlearn.freeze_to(-1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "0f63223ee2bfa9d05457908b28e59bcf2c904041"
      },
      "cell_type": "code",
      "source": "learn.lr_find()\nlearn.recorder.plot()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d6e91265fc44a500cba5b98c8df863acc3ca8b41"
      },
      "cell_type": "code",
      "source": "cb_dl = DataloaderCallback(learn, schedule_pars=(1.0, 0.05, 0, 10))\ncbs = [cb_dl]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "8869bf0a838bb863ce423769d02371ecaccd508e"
      },
      "cell_type": "code",
      "source": "learn.fit_one_cycle(4 , 1e-3, callbacks=cbs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "16f717c0127af50991cfaa3da28a5d3b945c1684"
      },
      "cell_type": "code",
      "source": "learn.unfreeze()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aa264f42b8195885f1f388445d126700b0bddd29"
      },
      "cell_type": "code",
      "source": "max_lr = 3e-4\nlrs = [max_lr/100, max_lr/10, max_lr]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "11a27543aac02d1829f5cfce2f2a3bd3f63aeed7"
      },
      "cell_type": "code",
      "source": "learn.fit_one_cycle(10, lrs, callbacks=cbs)\nlearn.save(f'{name}-stage2_unfz')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "45b00cce51bd3f20d3a06e670dcb0887e504a96e"
      },
      "cell_type": "code",
      "source": "learn.load(f'{name}-stage2_unfz');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7db7ce34c70ddc1712db64bfb9f92f6ab09ad62e"
      },
      "cell_type": "code",
      "source": "learn.recorder.plot_losses()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "31d68aa57fe121d6ff2ea336c34ad3c722b5a70d"
      },
      "cell_type": "code",
      "source": "cb_dl = DataloaderCallback(learn, schedule_pars=(0.05, 0.01, 0, 10))\ncbs = [cb_dl]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "84365bc64ee9a0721ac1388eaa8175e1ba39248f"
      },
      "cell_type": "code",
      "source": "learn.fit_one_cycle(10, lrs, callbacks=cbs)\nlearn.save(f'{name}-stage3_unfz')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c19280da6a71017bdde5adc21b8fddce0c2b77e"
      },
      "cell_type": "code",
      "source": "learn.fit_one_cycle(10, lrs, callbacks=cbs)\nlearn.save(f'{name}-stage4_unfz')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "907d93f72c9195f343defb70ba2097ee2fd1e7a8"
      },
      "cell_type": "code",
      "source": "learn.fit_one_cycle(10, lrs, callbacks=cbs)\nlearn.save(f'{name}-stage5_unfz')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cfda61725a5a9a82937970e4f75211d1f295b6c9"
      },
      "cell_type": "code",
      "source": "learn.fit_one_cycle(10, lrs, callbacks=cbs)\nlearn.save(f'{name}-stage6_unfz')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "718a3c7561aa51e5d7902c718b0fcc6a1f930da7"
      },
      "cell_type": "code",
      "source": "learn.fit_one_cycle(10, lrs, callbacks=cbs)\nlearn.save(f'{name}-stage7_unfz')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "efa1effc2bbed8d07aba9b913552ded9b4d5d7b3"
      },
      "cell_type": "code",
      "source": "learn.fit_one_cycle(10, lrs, callbacks=cbs)\nlearn.save(f'{name}-stage8_unfz')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f49bd41d195adb34deeb66e0617c48237679810"
      },
      "cell_type": "code",
      "source": "learn.fit_one_cycle(10, lrs, callbacks=cbs)\nlearn.save(f'{name}-stage9_unfz')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "83e090f8278044116508d8ca6c8676fd73f145d6"
      },
      "cell_type": "code",
      "source": "learn.fit_one_cycle(10, lrs, callbacks=cbs)\nlearn.save(f'{name}-stage10_unfz')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d798d591ea9f4d5c5a14a99b752bc038de6c192d"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4e9b4695761c9b282516f1cb4714add490050d7d"
      },
      "cell_type": "code",
      "source": "learn.load(f'{name}-stage9_unfz')\nlearn.model.cuda()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6dbddfedb90054e189326aba0ccf42aecf7f9ecf"
      },
      "cell_type": "code",
      "source": "new_whale_fns = set(df[df['Id']=='new_whale'].sample(frac = 1).Image.iloc[:1000])\n#new_whale_fns\nval_fns = set(df[df.sighting_count == 2].Image)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0508f2be224c3220ffa2a0c41bc8c7dfeb8ff302"
      },
      "cell_type": "code",
      "source": "val_fns = set(df[df.sighting_count == 2].Image)\nprint(len(val_fns) + len(new_whale_fns))\n\nclasses = df.Id.unique()\n\ndf = df.drop(columns = ['sighting_count'])\ndf.head(2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2120ab5ab3dcacf59852825ab673f227cfa40fad"
      },
      "cell_type": "code",
      "source": "\ndata = (\n    ImageItemList\n        .from_df(df, train_path, cols=['Image'])\n        .split_by_valid_func(lambda path: path2fn(path) in val_fns.union(new_whale_fns))\n        .label_from_func(lambda path: fn2label[path2fn(path)], classes=classes)\n        .add_test(ImageItemList.from_folder(test_path))\n        .transform(get_transforms(do_flip=False), size=SZ, resize_method=ResizeMethod.SQUISH)\n        .databunch(bs=BS, num_workers=NUM_WORKERS, path=root_path)\n        .normalize(imagenet_stats)\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "3a87303524c041736fd35e52d22cbd59804b5872"
      },
      "cell_type": "code",
      "source": "dist_mat, val_target, _ = cal_mat(learn.model, data.valid_dl, data.valid_dl, ds_with_target1=True, ds_with_target2=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "56ce8c0977654009fd3b0e110d3bd1329544ff24"
      },
      "cell_type": "code",
      "source": "%%time\ntargs = []\nfeats = []\nlearn.model.eval()\nwith torch.no_grad():\n    for ims, ts in data.valid_dl:\n        feats.append(learn.model.process_features(learn.model.cnn(ims)))  ##\n        targs.append(ts)\n\n    feats = torch.cat(feats)\n    print(feats.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "56cf455037a951cbb2cd9b50274ab22c1db8917c"
      },
      "cell_type": "code",
      "source": "feats",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc47dbfadf97ce95fea38071c670f616fe8ad9a4"
      },
      "cell_type": "code",
      "source": "%%time\nsims = []\nlearn.model.eval()\nwith torch.no_grad():\n    for feat in feats:\n        x1 = feats#.copy()\n        x2 = feat.unsqueeze(0).repeat(3570 ,1)\n        d1 = learn.model.calculate_distance(x1 , x2)\n        d2 = (x1 + x2)\n        d3 = (x1*x2)\n        d4 = (x1-x2)*(x1 - x2)\n        concat_layer = torch.cat([d1 ,d2,d3, d4]  ,dim = 1)\n        concat_layer = concat_layer.view( - 1, 1, num_features_model(learn.model.cnn) , 4)   ## no of channels is second dimension\n        concat_layer  = F.relu(learn.model.conv1(concat_layer.cuda()))\n        concat_layer = concat_layer.view(-1 ,1,32, num_features_model(learn.model.cnn)  )\n        concat_layer = F.relu(learn.model.conv2(concat_layer))\n        concat_layer_fn = concat_layer.view(-1 ,num_features_model(learn.model.cnn) )\n        #out = learn.model.head(concat_layer_fn)\n        predicted_similarity = learn.model.head(concat_layer_fn).sigmoid_()  #.cuda()\n        sims.append(predicted_similarity.squeeze())\n\n    \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b5fb7e323df740b955cff3cd6fe9200c187bc19a"
      },
      "cell_type": "code",
      "source": "new_whale_idx = np.where(classes == 'new_whale')[0][0]\nnew_whale_idx",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ff744445903289ee7082fc6dc714b313eea02ecf"
      },
      "cell_type": "code",
      "source": "top_5s = []\nlearn.model.eval()\nwith torch.no_grad():\n    for i, sim in enumerate(sims):\n        idxs = sim.argsort(descending=True)\n        probs = sim[idxs]\n        top_5 = []\n        for j, p in zip(idxs, probs):\n            if len(top_5) == 5: break\n            if j == i: continue   \n            predicted_class = data.valid_ds.y.items[j]\n            \"\"\"\n            we dont want to predict new whale for validation data \n            \"\"\"\n            if predicted_class == new_whale_idx: continue\n            if predicted_class not in top_5: top_5.append(predicted_class)\n        top_5s.append(top_5)\n\n    ## top 5 contains 5 best predicted classes ,w ith indices from classes dictionary",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "04b660a7e902fee06e944ae1ca7dc4900844f500"
      },
      "cell_type": "code",
      "source": "top_5s[:5]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92dea147d6f02f252d1c35be1becd130f5c547e2"
      },
      "cell_type": "code",
      "source": "\"\"\"\nmapk of validation data set without having new whales in predictions. \n\"\"\"\nmapk(data.valid_ds.y.items.reshape(-1,1), np.stack(top_5s), 5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1acd32b8060fbdd668888b45cd9fb3fed3033f6f"
      },
      "cell_type": "code",
      "source": "# %%time\n\"\"\"\ntrying to calcualte threshold probability for new whale, which maximises the mapk for validation data.\n\"\"\"\nlearn.model.eval()\nwith torch.no_grad():\n    for thresh in np.linspace(0.99, 1, 10):\n        top_5s = []\n        for i, sim in enumerate(sims):\n            idxs = sim.argsort(descending=True)\n            probs = sim[idxs]\n            top_5 = []\n            for j, p in zip(idxs, probs):\n                if new_whale_idx not in top_5 and p < thresh and len(top_5) < 5: top_5.append(new_whale_idx)\n                if len(top_5) == 5: break\n                if j == i: continue\n                predicted_class = data.valid_ds.y.items[j]\n                if predicted_class not in top_5: top_5.append(predicted_class)\n            top_5s.append(top_5)\n        print(thresh, mapk(data.valid_ds.y.items.reshape(-1,1), np.stack(top_5s), 5))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eefa2f7b2e3c4f0bfad4720933105bdba191af79"
      },
      "cell_type": "code",
      "source": "data = (\n    ImageItemList\n        .from_df(df, train_path, cols=['Image'])\n        .split_by_valid_func(lambda path: path2fn(path) in {'69823499d.jpg'}) # in newer version of the fastai library there is .no_split that could be used here\n        .label_from_func(lambda path: fn2label[path2fn(path)], classes=classes)\n        .add_test(ImageItemList.from_folder(test_path))\n        .transform(None, size=SZ, resize_method=ResizeMethod.SQUISH)\n        .databunch(bs=BS, num_workers=NUM_WORKERS, path=root_path)\n        .normalize(imagenet_stats)\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1c7ab39a1244ec5dc0153497a181d0c914113e4f"
      },
      "cell_type": "code",
      "source": "%%time\ntest_feats = []\nlearn.model.eval()\nwith torch.no_grad():\n    for ims, _ in data.test_dl:\n        test_feats.append(learn.model.process_features(learn.model.cnn(ims)))\n\n\n    test_feats = torch.cat(test_feats)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f0a0c68cf627b5d6496ca1d390e81acd6493b7ba"
      },
      "cell_type": "code",
      "source": "%%time\ntrain_feats = []\ntrain_class_idxs = []\nlearn.model.eval()\nwith torch.no_grad():\n    for ims, t in data.train_dl:\n        train_feats.append(learn.model.process_features(learn.model.cnn(ims)))\n        train_class_idxs.append(t)\n\n    train_class_idxs = torch.cat(train_class_idxs)\n    train_feats = torch.cat(train_feats)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5917229d93cfad6db0183e95e7d86b87ea370f00"
      },
      "cell_type": "code",
      "source": "len(train_class_idxs)\nlen(train_feats)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f8bd6f2960ae496ba7af48a227b46aab4318ce0c"
      },
      "cell_type": "code",
      "source": "train_feats",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9113c29f41ba0b69e41ef750e0c6da0e50a9775b"
      },
      "cell_type": "code",
      "source": "test_feats[0].expand(len(train_feats), 2048).shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ce62aee1e8260de36b85965410fa1c14abb79de6"
      },
      "cell_type": "code",
      "source": "train_feats = train_feats.cuda()\ntest_feats = test_feats.cuda()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4823e11f48d1300171d331126586a9e58c31b8f0"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee0a098b7de6f941ab079878c0544d27bad599d7"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f768b5db3aa245c5bb13d9a6716312c8b7d5770"
      },
      "cell_type": "code",
      "source": "%%time\ntorch.cuda.empty_cache()\nsims = []\nlearn.model.eval()\nwith torch.no_grad():\n    tmp_batch_size = 1000\n    for i, feat in enumerate(test_feats):\n        if i % 1000 == 0:\n            print(i, len(test_feats))\n        #dists = learn.model.calculate_distance(train_feats, feat.unsqueeze(0).repeat(len(train_feats), 1))\n        row_sim = []\n        for k in range(0, len(train_feats), tmp_batch_size):\n            x1 = train_feats[k:k+tmp_batch_size]\n            x2 = feat.expand(x1.shape)\n            d1 = learn.model.calculate_distance(x1 , x2)\n            d2 = (x1 + x2)\n            d3 = (x1*x2)\n            d4 = (x1-x2)*(x1 - x2)\n            concat_layer = torch.cat([d1 ,d2,d3, d4]  ,dim = 1)\n            concat_layer = concat_layer.view( - 1, 1, num_features_model(learn.model.cnn) , 4)   ## no of channels is second dimension\n            concat_layer  = F.relu(learn.model.conv1(concat_layer.cuda()))\n            concat_layer = concat_layer.view(-1 ,1,32, num_features_model(learn.model.cnn)  )\n            concat_layer = F.relu(learn.model.conv2(concat_layer))\n            concat_layer_fn = concat_layer.view(-1 ,num_features_model(learn.model.cnn) )\n            predicted_similarity = learn.model.head(concat_layer_fn).sigmoid_()  #.cuda()\n            row_sim.append(predicted_similarity)\n        row_sim = torch.cat(row_sim)\n        sims.append(row_sim.squeeze().detach().cpu())\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4bd768e7084208b028e4019a5092703f2932ec9d"
      },
      "cell_type": "code",
      "source": "\nthresh = 0.996 #0.95\n\ntop_5s = []\n\nfor sim in sims:\n    idxs = sim.argsort(descending = True)\n    probs = sim[idxs]\n    top_5 = []\n    \n    \n    for  i , p in zip(idxs , probs):\n        if new_whale_idx not in top_5 and p < thresh  and len(top_5) < 5:\n            top_5.append(new_whale_idx)\n        if len(top_5) ==5: break\n        #if i == new_whale_idx: continue\n        predicted_class = train_class_idxs[i]\n        #print(predicted_class)\n        if predicted_class == new_whale_idx: continue\n        if predicted_class not in top_5:\n            top_5.append(predicted_class)\n    top_5s.append(top_5)\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1d5990f0330439a7b24e5ab05eb27f2f05843929"
      },
      "cell_type": "code",
      "source": "len(top_5s)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9217825957a831d86bb8d3de82a2f128c56fdc6b"
      },
      "cell_type": "code",
      "source": "top_5_classes  = []\n\nfor top_5 in top_5s:\n    top_5_classes.append(' '.join([classes[t] for t in top_5]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f0d409a2f06b813e3a80054c2403b66854bb2a89"
      },
      "cell_type": "code",
      "source": "top_5_classes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "573cee7e98b06a636f0d14056331b6f4511f80f7"
      },
      "cell_type": "code",
      "source": "sub = pd.DataFrame({'Image': [path.name for path in data.test_ds.x.items]})\nsub['Id'] = top_5_classes\nsub.to_csv(f'../submission/{name}.csv', index=False) \n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a82e26f1fee6c232927d2542e366405af70e1984"
      },
      "cell_type": "code",
      "source": "pd.read_csv(f'../submission/{name}.csv').Id.str.split().apply(lambda x: x[0] == 'new_whale').mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b7d058c20a929de90d70c9ed2101d9d05464a280"
      },
      "cell_type": "code",
      "source": "#name = 'Ensembleing_resnet50_renet101_siamene_v1'\n! kaggle competitions submit -c humpback-whale-identification -f ../submission/martin_siamene_network_15k_training_images.csv -m \"resnet18 arch prob 1 for new whales\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ad38603d8296581280edf81adcb6cce8d255d5e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}