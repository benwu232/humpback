{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "# https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from fastai.vision import *\n",
    "from fastai.metrics import accuracy_thresh\n",
    "from fastai.basic_data import *\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "from fastai.callbacks.hooks import num_features_model, model_sizes\n",
    "from fastai.layers import BCEWithLogitsFlat\n",
    "from fastai.basic_train import Learner\n",
    "from skimage.util import montage\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "import re\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fastai\n",
    "# from fastprogress import force_console_behavior\n",
    "# import fastprogress\n",
    "# fastprogress.fastprogress.NO_BAR = True\n",
    "# master_bar, progress_bar = force_console_behavior()\n",
    "# fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posing the problem as a classification task is probably not ideal. We are asking our NN to learn to recognize a whale out of 5004 possible candidates based on what it has learned about the whales. That is a tall order.\n",
    "\n",
    "Instead, here we will try to pose the problem as a verification task. When presented with two images of whale flukes, we will ask the network - are the images of the same whale or of different whales? In particular, we will try to teach our network to learn features that can be useful in determining the similarity between whale images (hence the name of this approach - feature learning).\n",
    "\n",
    "This seems like a much easier task, at least in theory. Either way, no need to start with a relatively big CNN like resnet50. Let's see what mileage we can get out of resnet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path('../input')\n",
    "train_path = root_path/'train'\n",
    "test_path = root_path/'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new architecture calls for a new validation set, this time our validation set will consist of all whales that have exactly two images\n",
    "df = pd.read_csv(root_path/'train.csv')\n",
    "im_count = df[df.Id != 'new_whale'].Id.value_counts()\n",
    "im_count.name = 'sighting_count'\n",
    "df = df.join(im_count, on='Id')\n",
    "val_fns = set(df[df.sighting_count == 2].Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2570"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn2label = {row[1].Image: row[1].Id for row in df.iterrows()}\n",
    "path2fn = lambda path: re.search('\\w*\\.jpg$', path).group(0)\n",
    "\n",
    "name = f'res18-siamese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SZ = 224\n",
    "BS = 64\n",
    "NUM_WORKERS = 6\n",
    "SEED=0\n",
    "train_path = root_path/f'train_{SZ}'\n",
    "test_path = root_path/f'test_{SZ}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_block api creates categories based on classes it sees in the train set and\n",
    "# our val set contains whales whose ids do not appear in the train set\n",
    "classes = df.Id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    ImageItemList\n",
    "        .from_df(df[df.Id != 'new_whale'], train_path, cols=['Image'])\n",
    "        .split_by_valid_func(lambda path: path2fn(path) in val_fns)\n",
    "        .label_from_func(lambda path: fn2label[path2fn(path)], classes=classes)\n",
    "        .add_test(ImageItemList.from_folder(test_path))\n",
    "        .transform(get_transforms(do_flip=False), size=SZ, resize_method=ResizeMethod.SQUISH)\n",
    "#         .databunch(bs=BS, num_workers=NUM_WORKERS, path='data')\n",
    "#         .normalize(imagenet_stats)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am still using the ImageItemList even though I will create my own datasets. Why? Because I want to reuse the functionality that is already there (creating datasets from files, augmentations, resizing, etc).\n",
    "\n",
    "I realize the code is neither clean nor elegant but for the time being I am happy with this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_even(num): return num % 2 == 0\n",
    "\n",
    "class TwoImDataset(Dataset):\n",
    "    def __init__(self, ds):\n",
    "        self.ds = ds\n",
    "        self.whale_ids = ds.y.items\n",
    "    def __len__(self):\n",
    "        return 2 * len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        if is_even(idx):\n",
    "            return self.sample_same(idx // 2)\n",
    "        else: return self.sample_different((idx-1) // 2)\n",
    "    def sample_same(self, idx):\n",
    "        whale_id = self.whale_ids[idx]        \n",
    "        candidates = list(np.where(self.whale_ids == whale_id)[0])\n",
    "        candidates.remove(idx) # dropping our current whale - we don't want to compare against an identical image!\n",
    "        \n",
    "        if len(candidates) == 0: # oops, there is only a single whale with this id in the dataset\n",
    "            return self.sample_different(idx)\n",
    "        \n",
    "        np.random.shuffle(candidates)\n",
    "        return self.construct_example(self.ds[idx][0], self.ds[candidates[0]][0], 1)\n",
    "    def sample_different(self, idx):\n",
    "        whale_id = self.whale_ids[idx]\n",
    "        candidates = list(np.where(self.whale_ids != whale_id)[0])\n",
    "        np.random.shuffle(candidates)\n",
    "        return self.construct_example(self.ds[idx][0], self.ds[candidates[0]][0], 0)\n",
    "    \n",
    "    def construct_example(self, im_A, im_B, class_idx):\n",
    "        return [im_A, im_B], class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(\n",
    "    TwoImDataset(data.train),\n",
    "    batch_size=BS,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "valid_dl = DataLoader(\n",
    "    TwoImDataset(data.valid),\n",
    "    batch_size=BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "data_bunch = ImageDataBunch(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_batch(batch):\n",
    "    stat_tensors = [torch.tensor(l).cuda() for l in imagenet_stats]\n",
    "    return [normalize(batch[0][0], *stat_tensors), normalize(batch[0][1], *stat_tensors)], batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bunch.add_tfm(normalize_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functional import seq\n",
    "\n",
    "class SiameseNetwork1(nn.Module):\n",
    "    def __init__(self, arch=models.resnet18):\n",
    "        super().__init__() \n",
    "        self.cnn = create_body(arch)\n",
    "        self.head = nn.Linear(num_features_model(self.cnn), 1)\n",
    "        \n",
    "    def forward(self, im_A, im_B):\n",
    "        # dl - distance layer\n",
    "        x1, x2 = seq(im_A, im_B).map(self.cnn).map(self.process_features)\n",
    "        dl = self.calculate_distance(x1, x2)\n",
    "        out = self.head(dl)\n",
    "        return out\n",
    "    \n",
    "    def process_features(self, x): return x.reshape(*x.shape[:2], -1).max(-1)[0]\n",
    "    def calculate_distance(self, x1, x2): return (x1 - x2).abs()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, arch=models.resnet18):\n",
    "        super().__init__()\n",
    "        self.cnn = create_body(arch)\n",
    "        self.fc = nn.Linear(num_features_model(self.cnn), 1)\n",
    "\n",
    "    def im2emb(self, batch):\n",
    "        x = self.cnn(batch)\n",
    "        x = self.process_features(x)\n",
    "        return x\n",
    "\n",
    "    def forward1(self, im1, im2):\n",
    "        x1 = self.cnn(im1)\n",
    "        x1 = self.process_features(x1)\n",
    "        x2 = self.cnn(im2)\n",
    "        x2 = self.process_features(x2)\n",
    "        dl = self.distance(x1, x2)\n",
    "        out = self.fc(dl)\n",
    "        return out\n",
    "\n",
    "    def forward(self, im1, im2):\n",
    "        x1 = self.cnn(im1)\n",
    "        x1 = self.process_features(x1)\n",
    "        x2 = self.cnn(im2)\n",
    "        x2 = self.process_features(x2)\n",
    "        dl = self.distance(x1, x2).mean(dim=1)\n",
    "        return dl\n",
    "\n",
    "    def process_features(self, x):\n",
    "        return x.reshape(*x.shape[:2], -1).max(-1)[0]\n",
    "\n",
    "    def distance(self, x1, x2):\n",
    "        return (x1 - x2).abs()\n",
    "\n",
    "    def similarity(self, x1, x2):\n",
    "        dl = self.distance(x1, x2)\n",
    "        logit = self.fc(dl)\n",
    "        return torch.sigmoid(logit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss\n",
    "    Takes embeddings of two samples and a target label == 1 if samples are from the same class and label == 0 otherwise\n",
    "    \"\"\"\n",
    "    def __init__(self, margin):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, distances, target, size_average=True):\n",
    "        losses = target.float() * distances + (1 - target).float() * torch.relu(self.margin - distances)\n",
    "        return losses.mean() if size_average else losses.sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I include two slightly different siamese networks. I leave the code commented out and choose to use the one above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functional import seq\n",
    "\n",
    "# def cnn_activations_count(model):\n",
    "#     _, ch, h, w = model_sizes(create_body(models.resnet18), (SZ, SZ))[-1]\n",
    "#     return ch * h * w\n",
    "\n",
    "# class SiameseNetwork(nn.Module):\n",
    "#     def __init__(self, lin_ftrs=2048, arch=models.resnet18):\n",
    "#         super().__init__() \n",
    "#         self.cnn = create_body(arch)\n",
    "#         self.fc1 = nn.Linear(cnn_activations_count(self.cnn), lin_ftrs)\n",
    "#         self.fc2 = nn.Linear(lin_ftrs, 1)\n",
    "        \n",
    "#     def forward(self, im_A, im_B):\n",
    "#         x1, x2 = seq(im_A, im_B).map(self.cnn).map(self.process_features).map(self.fc1)\n",
    "#         dl = self.calculate_distance(x1.sigmoid(), x2.sigmoid())\n",
    "#         out = self.fc2(dl)\n",
    "#         return out\n",
    "    \n",
    "#     def calculate_distance(self, x1, x2): return (x1 - x2).abs_()\n",
    "#     def process_features(self, x): return x.reshape(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functional import seq\n",
    "\n",
    "# def cnn_activations_count(model):\n",
    "#     _, ch, h, w = model_sizes(create_body(models.resnet18), (SZ, SZ))[-1]\n",
    "#     return ch * h * w\n",
    "\n",
    "# class SiameseNetwork(nn.Module):\n",
    "#     def __init__(self, lin_ftrs=2048, pool_to=3, arch=models.resnet18, pooling_layer=nn.AdaptiveMaxPool2d):\n",
    "#         super().__init__() \n",
    "#         self.cnn = create_body(arch)\n",
    "#         self.pool = pooling_layer(pool_to)\n",
    "#         self.fc1 = nn.Linear(num_features_model(self.cnn) * pool_to**2, lin_ftrs)\n",
    "#         self.fc2 = nn.Linear(lin_ftrs, 1)\n",
    "        \n",
    "#     def forward(self, im_A, im_B):\n",
    "#         x1, x2 = seq(im_A, im_B).map(self.cnn).map(self.pool).map(self.process_features).map(self.fc1)\n",
    "#         dl = self.calculate_distance(x1.sigmoid(), x2.sigmoid())\n",
    "#         out = self.fc2(dl)\n",
    "#         return out\n",
    "    \n",
    "#     def calculate_distance(self, x1, x2): return (x1 - x2).abs_()\n",
    "#     def process_features(self, x): return x.reshape(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data_bunch, SiameseNetwork(), loss_func=BCEWithLogitsFlat(), metrics=[lambda preds, targs: accuracy_thresh(preds.squeeze(), targs, sigmoid=False)])\n",
    "#learn = Learner(data_bunch, SiameseNetwork(), loss_func=ContrastiveLoss(margin=1.0), metrics=[lambda preds, targs: accuracy_thresh(preds.squeeze(), targs, sigmoid=False)])\n",
    "#learn = Learner(data_bunch, SiameseNet(), loss_func=BCEWithLogitsFlat(), metrics=[lambda preds, targs: accuracy_thresh(preds.squeeze(), targs, sigmoid=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.split([learn.model.cnn[:6], learn.model.cnn[6:], learn.model.fc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(4, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'{name}-stage-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 5e-4\n",
    "lrs = [max_lr/100, max_lr/10, max_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'{name}-stage-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is not doing that well - out of presented pairs it gets roughly 10% of examples wrong. I also did a cursory error analysis (not shown here for the sake of brevity) and the model is not doing that great at all.\n",
    "\n",
    "How can this be? Maybe the nearly absolute positional invariance through the use of global max pooling is not working that well. Maybe there is a bug somewhere? Maybe the model has not been trained for long enough or lacks capacity?\n",
    "\n",
    "If I do continue to work on this I will definitely take a closer look at each of the angles I list above. For the time being, let's try to predict on the validation set and finish off with making a submission.\n",
    "\n",
    "The predicting part is where the code gets really messy. That is good enough for now though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'{name}-stage-2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_whale_fns = set(df[df.Id == 'new_whale'].sample(frac=1).Image.iloc[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    ImageItemList\n",
    "        .from_df(df, train_path, cols=['Image'])\n",
    "        .split_by_valid_func(lambda path: path2fn(path) in val_fns.union(new_whale_fns))\n",
    "        .label_from_func(lambda path: fn2label[path2fn(path)], classes=classes)\n",
    "        .add_test(ImageItemList.from_folder(test_path))\n",
    "        .transform(get_transforms(do_flip=False), size=SZ, resize_method=ResizeMethod.SQUISH)\n",
    "        .databunch(bs=BS, num_workers=NUM_WORKERS, path=root_path)\n",
    "        .normalize(imagenet_stats)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3570"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21791"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoryList (21791 items)\n",
       "[Category w_f48451c, Category w_c3d896a, Category w_20df2c5, Category new_whale, Category new_whale]...\n",
       "Path: ../input/train_224"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_ds.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import siamese_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist_pos_max = 0.7686152458190918, dist_neg_min = 0.21202769875526428\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3572 is out of bounds for axis 0 with size 3570",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-4bcf6ce0b96f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m map5, pos_dist_max, neg_dist_min = siamese_validate(data.valid_dl, learn.model, data.train_dl,\n\u001b[1;32m      2\u001b[0m                                                             \u001b[0mpos_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_idx2class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                                             target_idx2class=data.valid_ds.y)\n\u001b[0m",
      "\u001b[0;32m~/work/whale/whale/utils.py\u001b[0m in \u001b[0;36msiamese_validate\u001b[0;34m(val_dl, model, rf_dl, pos_mask, ref_idx2class, target_idx2class)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;31m# cal map5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0mtop5_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_mapk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_target_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_idx2class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref_idx2class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_idx2class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_idx2class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'map5 = {map5}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmap5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_pos_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_neg_min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/whale/whale/utils.py\u001b[0m in \u001b[0;36mcal_mapk\u001b[0;34m(data_matrix, targets, target_idx2class, k, average, threshold, ref_idx2class, descending)\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0mtopk_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0mc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mtarget_idx2class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mclass_str\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmapk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                         \u001b[0mmapk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/fastai/data_block.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idxs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxtra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/fastai/data_block.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mCategory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3572 is out of bounds for axis 0 with size 3570"
     ]
    }
   ],
   "source": [
    "map5, pos_dist_max, neg_dist_min = siamese_validate(data.valid_dl, learn.model, data.train_dl,\n",
    "                                                            pos_mask=[0], ref_idx2class=data.train_ds.y,\n",
    "                                                            target_idx2class=data.valid_ds.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.58 s, sys: 992 ms, total: 3.57 s\n",
      "Wall time: 4.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "targs = []\n",
    "feats = []\n",
    "learn.model.eval()\n",
    "for ims, ts in data.valid_dl:\n",
    "    feats.append(learn.model.im2emb(ims).detach().cpu())\n",
    "    targs.append(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = torch.cat(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sims = []\n",
    "for feat in feats:\n",
    "    dists = learn.model.distance(feats, feat.unsqueeze(0).repeat(3570, 1))\n",
    "    dists = dists.detach().cpu()\n",
    "    #predicted_similarity = learn.model.head(dists.cuda()).sigmoid_()\n",
    "    sims.append(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sims[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_whale_idx = np.where(classes == 'new_whale')[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims[0].argsort(descending=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "top_5s = []\n",
    "for i, sim in enumerate(sims):\n",
    "    idxs = sim.argsort(descending=True)\n",
    "    probs = sim[idxs]\n",
    "    top_5 = []\n",
    "    for j, p in zip(idxs, probs):\n",
    "        if len(top_5) == 5: break\n",
    "        if j == i: continue\n",
    "        predicted_class = data.valid_ds.y.items[j]\n",
    "        if j == predicted_class: continue\n",
    "        if predicted_class not in top_5: top_5.append(predicted_class)\n",
    "    top_5s.append(top_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without predicting new_whale\n",
    "mapk(data.valid_ds.y.items.reshape(-1,1), np.stack(top_5s), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for thresh in np.linspace(0.98, 1, 10):\n",
    "    top_5s = []\n",
    "    for i, sim in enumerate(sims):\n",
    "        idxs = sim.argsort(descending=True)\n",
    "        probs = sim[idxs]\n",
    "        top_5 = []\n",
    "        for j, p in zip(idxs, probs):\n",
    "            if new_whale_idx not in top_5 and p < thresh and len(top_5) < 5: top_5.append(new_whale_idx)\n",
    "            if len(top_5) == 5: break\n",
    "            if j == new_whale_idx or j == i: continue\n",
    "            predicted_class = data.valid_ds.y.items[j]\n",
    "            if predicted_class not in top_5: top_5.append(predicted_class)\n",
    "        top_5s.append(top_5)\n",
    "    print(thresh, mapk(data.valid_ds.y.items.reshape(-1,1), np.stack(top_5s), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many reasons why the best threshold here might not carry over to what would make sense on the test set. It is some indication though of how our model is doing and a useful data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7960"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    ImageItemList\n",
    "        .from_df(df, train_path, cols=['Image'])\n",
    "        .split_by_valid_func(lambda path: path2fn(path) in {'69823499d.jpg'}) # in newer version of the fastai library there is .no_split that could be used here\n",
    "        .label_from_func(lambda path: fn2label[path2fn(path)], classes=classes)\n",
    "        .add_test(ImageItemList.from_folder(test_path))\n",
    "        .transform(None, size=SZ, resize_method=ResizeMethod.SQUISH)\n",
    "        .databunch(bs=BS, num_workers=NUM_WORKERS, path=root_path)\n",
    "        .normalize(imagenet_stats)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.68 s, sys: 1.76 s, total: 7.44 s\n",
      "Wall time: 8.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_feats = []\n",
    "learn.model.eval()\n",
    "for ims, _ in data.test_dl:\n",
    "    test_feats.append(learn.model.process_features(learn.model.cnn(ims)).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 s, sys: 4.76 s, total: 22.7 s\n",
      "Wall time: 23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_feats = []\n",
    "train_class_idxs = []\n",
    "learn.model.eval()\n",
    "for ims, t in data.train_dl:\n",
    "    train_feats.append(learn.model.process_features(learn.model.cnn(ims)).detach().cpu())\n",
    "    train_class_idxs.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_idxs = torch.cat(train_class_idxs)\n",
    "train_feats = torch.cat(train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feats = torch.cat(test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25344, 512])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 ms, sys: 192 ms, total: 228 ms\n",
      "Wall time: 457 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_feats = []\n",
    "learn.model.eval()\n",
    "for ims, _ in data.test_dl:\n",
    "    test_emb = (learn.model.im2emb(ims))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 512])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sims = []\n",
    "for feat in test_feats:\n",
    "    dists = learn.model.distance(train_feats, feat.unsqueeze(0).repeat(25344, 1))\n",
    "    #predicted_similarity = learn.model.head(dists.cuda()).sigmoid_()\n",
    "    sims.append(predicted_similarity.squeeze().detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "thresh = 1\n",
    "\n",
    "top_5s = []\n",
    "for sim in sims:\n",
    "    idxs = sim.argsort(descending=True)\n",
    "    probs = sim[idxs]\n",
    "    top_5 = []\n",
    "    for i, p in zip(idxs, probs):\n",
    "        if new_whale_idx not in top_5 and p < thresh and len(top_5) < 5: top_5.append(new_whale_idx)\n",
    "        if len(top_5) == 5: break\n",
    "        if i == new_whale_idx: continue\n",
    "        predicted_class = train_class_idxs[i]\n",
    "        if predicted_class not in top_5: top_5.append(predicted_class)\n",
    "    top_5s.append(top_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_classes = []\n",
    "for top_5 in top_5s:\n",
    "    top_5_classes.append(' '.join([classes[t] for t in top_5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_classes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'Image': [path.name for path in data.test_ds.x.items]})\n",
    "sub['Id'] = top_5_classes\n",
    "sub.to_csv(f'../submission/{name}.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f'../submission/{name}.csv.gz').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f'../submission/{name}.csv.gz').Id.str.split().apply(lambda x: x[0] == 'new_whale').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c humpback-whale-identification -f subs/{name}.csv.gz -m \"{name}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
