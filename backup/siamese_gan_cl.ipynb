{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "66456b04405182f1e5583058307f702b130fb40e"
   },
   "source": [
    "## This is my attempt to use siamese with gan idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "906946c5539307d4d006ac922f6516c8ca73973d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "use_kaggle = 0\n",
    "\n",
    "if use_kaggle:\n",
    "    #!pip install git+https://github.com/fastai/fastai.git\n",
    "    !git clone https://github.com/benwu232/humpback\n",
    "    import sys\n",
    "     # Add directory holding utility functions to path to allow importing utility funcitons\n",
    "    #sys.path.insert(0, '/kaggle/working/protein-atlas-fastai')\n",
    "    sys.path.append('/kaggle/humback/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "de5edc472d60a4712451c4697bcdac9491052942"
   },
   "outputs": [],
   "source": [
    "# Suppress annoying stderr output when importing keras.\n",
    "import sys\n",
    "import platform\n",
    "old_stderr = sys.stderr\n",
    "sys.stderr = open('/dev/null' if platform.system() != 'Windows' else 'nul', 'w')\n",
    "\n",
    "sys.stderr = old_stderr\n",
    "\n",
    "import random\n",
    "from scipy.ndimage import affine_transform\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "# Determise the size of each image\n",
    "from os.path import isfile\n",
    "from PIL import Image as pil_image\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from fastai.vision import *\n",
    "from fastai.metrics import accuracy_thresh\n",
    "from fastai.basic_data import *\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "from fastai.callbacks.hooks import num_features_model, model_sizes\n",
    "from fastai.layers import BCEWithLogitsFlat\n",
    "from fastai.basic_train import Learner\n",
    "from skimage.util import montage\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "import re\n",
    "\n",
    "if use_kaggle:\n",
    "    from humpback.utils import *\n",
    "else:\n",
    "    from utils import *\n",
    "    \n",
    "from IPython.core.debugger import set_trace\n",
    "#from functional import seq\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "93bb20436b38d8f58ad0d878e279c06ea6f0801e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.44.dev0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"shell_port\": 47332,\n",
      "  \"iopub_port\": 33229,\n",
      "  \"stdin_port\": 60831,\n",
      "  \"control_port\": 40941,\n",
      "  \"hb_port\": 43430,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"key\": \"12129e0f-da88447a9c8647dfdce78cf0\",\n",
      "  \"transport\": \"tcp\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-0cf0cb30-675b-418d-9c43-3e93783a221a.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "2362a475b45c5d126eaf17106cdbfddec1bb3cb2"
   },
   "outputs": [],
   "source": [
    "root_path = Path('../input')\n",
    "train_path = root_path/'train'\n",
    "test_path = root_path/'test'\n",
    "learn_path = Path('../')\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "5cb2990ac2fe2a4f733535b1c755afbef35b6f96"
   },
   "outputs": [],
   "source": [
    "name = f'siamese_resnet18_224'\n",
    "\n",
    "#arch = models.resnet50\n",
    "arch = models.resnet18\n",
    "im_size = 224\n",
    "train_batch_size = 64\n",
    "val_batch_size = 128\n",
    "if use_kaggle:\n",
    "    dl_workers = 0\n",
    "else:\n",
    "    dl_workers = 6\n",
    "SEED=0\n",
    "emb_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "f6d315086f994303b0f4d67b4c968621eb437a29"
   },
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('../input/train.csv')\n",
    "df_new = df0[df0.Id == 'new_whale']\n",
    "df_known = df0[df0.Id != 'new_whale']\n",
    "train_list, val_list = split_whale_set(df0, nth_fold=0, new_whale_method=1, seed=1)\n",
    "\n",
    "im_count = df0[df0.Id != 'new_whale'].Id.value_counts()\n",
    "im_count.name = 'sighting_count'\n",
    "ex_df = df0.join(im_count, on='Id')\n",
    "\n",
    "path2fn = lambda path: re.search('\\w*\\.jpg$', path).group(0)\n",
    "fn2label = {row[1].Image: row[1].Id for row in df0.iterrows()}\n",
    "class_dict = make_whale_class_dict(df0)\n",
    "file_lut = df0.set_index('Image').to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "12ef302f15102e4b78bda45359e95e9f0ff61a68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5471"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "d203f911cec27d8decb301936c2da0b5e1f09252",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im_tfms = get_transforms(do_flip=False, max_zoom=1, max_warp=0, max_rotate=2)\n",
    "\n",
    "data = (\n",
    "    ImageItemList\n",
    "        # .from_df(df_known, 'data/train', cols=['Image'])\n",
    "        .from_folder(train_path)\n",
    "        # .split_by_idxs(train_item_list, val_item_list)\n",
    "        .split_by_valid_func(lambda path: path2fn(str(path)) in val_list)\n",
    "        # .split_by_idx(val_list)\n",
    "        # .random_split_by_pct(seed=SEED)\n",
    "        .label_from_func(lambda path: fn2label[path2fn(str(path))])\n",
    "        #.add_test(ImageItemList.from_folder(test_path))\n",
    "        #.transform([None, None], size=im_size, resize_method=ResizeMethod.SQUISH)\n",
    "        .transform(im_tfms, size=im_size, resize_method=ResizeMethod.SQUISH)\n",
    "        .databunch(bs=train_batch_size, num_workers=dl_workers, path=root_path)\n",
    "        #.normalize(imagenet_stats)\n",
    ")\n",
    "\n",
    "data.add_tfm(normalize_batch)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    SiameseDs(data.train_ds, data.train_dl),\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_siamese,\n",
    "    num_workers=dl_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "ed8a09f69418db00aa1ba2daa3d8986d9a242598"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wb/T08/study/fastai/fastai/basic_data.py:243: UserWarning: It's not possible to collate samples of your dataset together in a batch.\n",
      "Shapes of the inputs/targets:\n",
      "[[torch.Size([3, 326, 572]), torch.Size([3, 788, 1050]), torch.Size([3, 179, 312]), torch.Size([3, 343, 640]), torch.Size([3, 441, 1050]), torch.Size([3, 445, 778]), torch.Size([3, 407, 700]), torch.Size([3, 684, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 191, 1050]), torch.Size([3, 466, 815]), torch.Size([3, 342, 513]), torch.Size([3, 337, 1050]), torch.Size([3, 450, 1050]), torch.Size([3, 386, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 259, 453]), torch.Size([3, 678, 1017]), torch.Size([3, 600, 1050]), torch.Size([3, 525, 1050]), torch.Size([3, 220, 366]), torch.Size([3, 525, 1050]), torch.Size([3, 450, 1050]), torch.Size([3, 425, 705]), torch.Size([3, 600, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 415, 1050]), torch.Size([3, 251, 640]), torch.Size([3, 466, 1050]), torch.Size([3, 251, 593]), torch.Size([3, 300, 525]), torch.Size([3, 584, 879]), torch.Size([3, 600, 1050]), torch.Size([3, 544, 954]), torch.Size([3, 500, 700]), torch.Size([3, 648, 1697]), torch.Size([3, 450, 1050]), torch.Size([3, 362, 1050]), torch.Size([3, 478, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 413, 875]), torch.Size([3, 504, 1050]), torch.Size([3, 652, 1000]), torch.Size([3, 450, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 590, 886]), torch.Size([3, 600, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 326, 1050]), torch.Size([3, 254, 381]), torch.Size([3, 457, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 450, 1050]), torch.Size([3, 525, 1050]), torch.Size([3, 240, 1050]), torch.Size([3, 702, 1050]), torch.Size([3, 500, 700]), torch.Size([3, 126, 457]), torch.Size([3, 591, 1050]), torch.Size([3, 386, 700]), torch.Size([3, 500, 700]), torch.Size([3, 389, 1050])], [(), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), ()]]\n",
      "  warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can deactivate this warning by passing `no_check=True`.\n",
      "You can deactivate this warning by passing `no_check=True`.\n",
      "12158 3539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wb/T08/study/fastai/fastai/basic_data.py:243: UserWarning: It's not possible to collate samples of your dataset together in a batch.\n",
      "Shapes of the inputs/targets:\n",
      "[[torch.Size([3, 699, 1050]), torch.Size([3, 498, 873]), torch.Size([3, 635, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 773, 1050]), torch.Size([3, 352, 1050]), torch.Size([3, 674, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 380, 1050]), torch.Size([3, 714, 1050]), torch.Size([3, 306, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 525, 1050]), torch.Size([3, 477, 1050]), torch.Size([3, 469, 1020]), torch.Size([3, 242, 1050]), torch.Size([3, 387, 1050]), torch.Size([3, 625, 937]), torch.Size([3, 429, 1050]), torch.Size([3, 725, 1016]), torch.Size([3, 547, 957]), torch.Size([3, 325, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 591, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 415, 1050]), torch.Size([3, 681, 1021]), torch.Size([3, 423, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 309, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 232, 696]), torch.Size([3, 600, 1050]), torch.Size([3, 695, 1050]), torch.Size([3, 573, 1012]), torch.Size([3, 293, 1050]), torch.Size([3, 352, 926]), torch.Size([3, 394, 1050]), torch.Size([3, 525, 1050]), torch.Size([3, 363, 1050]), torch.Size([3, 312, 1050]), torch.Size([3, 675, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 717, 972]), torch.Size([3, 700, 1050]), torch.Size([3, 361, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 450, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 310, 1050]), torch.Size([3, 791, 1050]), torch.Size([3, 348, 1050]), torch.Size([3, 345, 950]), torch.Size([3, 296, 1050]), torch.Size([3, 349, 611]), torch.Size([3, 273, 637]), torch.Size([3, 701, 1050]), torch.Size([3, 292, 638]), torch.Size([3, 317, 1034]), torch.Size([3, 153, 403]), torch.Size([3, 700, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 740, 1050]), torch.Size([3, 373, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 407, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 499, 1050]), torch.Size([3, 167, 756]), torch.Size([3, 238, 786]), torch.Size([3, 700, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 804, 1050]), torch.Size([3, 253, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 525, 1050]), torch.Size([3, 652, 1000]), torch.Size([3, 450, 1050]), torch.Size([3, 592, 1050]), torch.Size([3, 525, 1050]), torch.Size([3, 658, 1050]), torch.Size([3, 336, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 353, 1050]), torch.Size([3, 244, 366]), torch.Size([3, 498, 807]), torch.Size([3, 365, 1050]), torch.Size([3, 599, 900]), torch.Size([3, 336, 787]), torch.Size([3, 600, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 618, 1050]), torch.Size([3, 320, 1050]), torch.Size([3, 237, 552]), torch.Size([3, 211, 371]), torch.Size([3, 377, 775]), torch.Size([3, 600, 1050]), torch.Size([3, 695, 5959]), torch.Size([3, 323, 713]), torch.Size([3, 269, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 346, 1050]), torch.Size([3, 410, 894]), torch.Size([3, 700, 1800]), torch.Size([3, 525, 1050]), torch.Size([3, 440, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 401, 1050]), torch.Size([3, 333, 1050]), torch.Size([3, 450, 1050]), torch.Size([3, 345, 604]), torch.Size([3, 700, 1050]), torch.Size([3, 700, 1050]), torch.Size([3, 600, 1050]), torch.Size([3, 790, 1010]), torch.Size([3, 700, 1050]), torch.Size([3, 464, 2287]), torch.Size([3, 588, 2091]), torch.Size([3, 270, 1050])], [(), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), ()]]\n",
      "  warn(message)\n"
     ]
    }
   ],
   "source": [
    "data_v = (\n",
    "    ImageItemList\n",
    "        # .from_df(df_known, 'data/train', cols=['Image'])\n",
    "        .from_folder(train_path)\n",
    "        # .split_by_idxs(train_item_list, val_item_list)\n",
    "        .split_by_valid_func(lambda path: path2fn(str(path)) in val_list)\n",
    "        # .split_by_idx(val_list)\n",
    "        # .random_split_by_pct(seed=SEED)\n",
    "        .label_from_func(lambda path: fn2label[path2fn(str(path))])\n",
    "        .add_test(ImageItemList.from_folder(test_path))\n",
    "        #.transform([None, None], size=im_size, resize_method=ResizeMethod.SQUISH)\n",
    "        #.transform(im_tfms, size=im_size, resize_method=ResizeMethod.SQUISH)\n",
    "        .databunch(bs=train_batch_size, num_workers=dl_workers, path=root_path)\n",
    "        .normalize(imagenet_stats)\n",
    ")\n",
    "\n",
    "#v = SimpleDataset(data.valid)\n",
    "valid_dl = DataLoader(\n",
    "    SimpleDataset(data_v.valid_ds),\n",
    "    batch_size=val_batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=dl_workers\n",
    ")\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    SimpleDataset(data_v.test_ds),\n",
    "    batch_size=val_batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=dl_workers\n",
    ")\n",
    "\n",
    "data_ref = (\n",
    "    ImageItemList\n",
    "        .from_df(df_known, train_path, cols=['Image'])\n",
    "        #.from_folder(train_path)\n",
    "        # .split_by_idxs(train_item_list, val_item_list)\n",
    "        .split_by_valid_func(lambda path: path2fn(str(path)) in val_list)\n",
    "        # .split_by_idx(val_list)\n",
    "        # .random_split_by_pct(seed=SEED)\n",
    "        .label_from_func(lambda path: fn2label[path2fn(str(path))])\n",
    "        #.add_test(ImageItemList.from_folder(test_path))\n",
    "        #.transform([None, None], size=im_size, resize_method=ResizeMethod.SQUISH)\n",
    "        #.transform(im_tfms, size=im_size, resize_method=ResizeMethod.SQUISH)\n",
    "        .databunch(bs=val_batch_size, num_workers=dl_workers, path=root_path)\n",
    "        .normalize(imagenet_stats)\n",
    ")\n",
    "print(len(data_ref.train_ds), len(data_ref.valid_ds))\n",
    "\n",
    "ref_dl = DataLoader(\n",
    "        SimpleDataset(data_ref.train_ds),\n",
    "        batch_size=val_batch_size,\n",
    "        shuffle=False,\n",
    "        #collate_fn=siamese_collate,\n",
    "        num_workers=dl_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "3f8f2d153e9970300574e135c523e1e43b740259"
   },
   "outputs": [],
   "source": [
    "data_bunch = ImageDataBunch(train_dl, valid_dl, fix_dl=ref_dl, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "168c45a50e0ea160109e8bc6c2bcb21c8ddc8fd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNet(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): AdaptiveConcatPool2d(\n",
       "      (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "      (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "    )\n",
       "    (1): Flatten()\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Dropout(p=0.2)\n",
       "    (4): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#siamese = SiameseNetwork(arch=arch)\n",
    "siamese = SiameseNet(emb_len=emb_len, arch=arch, forward_type='similarity', drop_rate=0.2)\n",
    "siamese.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "631ad12f3ed69a96e12d4d80ea171e372ad18eb1"
   },
   "outputs": [],
   "source": [
    "learn = LearnerEx(data_bunch,\n",
    "                  siamese,\n",
    "                  enable_validate=False,\n",
    "                  loss_func=BCEWithLogitsFlat(),\n",
    "                  path=learn_path,\n",
    "                  #loss_func=contrastive_loss,\n",
    "                  metrics=[lambda preds, targs: accuracy_thresh(preds.squeeze(), targs, sigmoid=False)]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "b04638dc94d032ab52162cc29d44e0bc55ae9530"
   },
   "outputs": [],
   "source": [
    "learn.split([learn.model.cnn[:6], learn.model.cnn[6:], learn.model.fc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "596b9aad355f1fce6be5989a717b71c5fab75b8f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 03:19 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th><lambda></th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>0.690777</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-1)\n",
    "learn.fit_one_cycle(1)\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "1cf65ceea3c1d43f351b2392e755a154774e24a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR plotting ...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x7fd4027c18c8>: attribute lookup <lambda> on __main__ failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-41609881ff9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0menable_lr_find\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LR plotting ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lr_find.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/wb/T08/study/fastai/fastai/train.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(learn, start_lr, end_lr, num_it, stop_div, wd)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_it\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m def to_fp16(learn:Learner, loss_scale:float=None, max_noskip:int=1000, dynamic:bool=False, clip:float=None,\n",
      "\u001b[0;32m/media/wb/backup/work/whale/humpback/utils.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         fit(epochs, self.model, self.loss_func, opt=self.opt, data=self.data, metrics=self.metrics,\n\u001b[0;32m--> 917\u001b[0;31m             callbacks=self.callbacks + callbacks, enable_validate=self.enable_validate)\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/wb/backup/work/whale/humpback/utils.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_func, opt, data, callbacks, metrics, enable_validate)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/wb/T08/study/fastai/fastai/callback.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, exception)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;34m\"Handle end of training, `exception` is an `Exception` or False if no exceptions during training.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAverageMetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/wb/T08/study/fastai/fastai/callback.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, cb_name, call_mets, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;34m\"Call through to all of the `CallbakHandler` functions.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcall_mets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'on_{cb_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'on_{cb_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/wb/T08/study/fastai/fastai/callback.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;34m\"Call through to all of the `CallbakHandler` functions.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcall_mets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'on_{cb_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'on_{cb_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/wb/T08/study/fastai/fastai/callbacks/lr_finder.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# restore the valid_dl we turned off on `__init__`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_dl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tmp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/wb/T08/study/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, name, device, strict, with_opt, purge)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mPathOrStr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_opt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpurge\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;34m\"Load model and optimizer state (if `with_opt`) `name` from `self.model_dir` using `device`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mpurge\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpurge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34mf'{name}.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/wb/T08/study/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mpurge\u001b[0;34m(self, clear_opt)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cb_state'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'tmp.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclear_opt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdelattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'opt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'callbacks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdelattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0mserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_storages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x7fd4027c18c8>: attribute lookup <lambda> on __main__ failed"
     ]
    }
   ],
   "source": [
    "enable_lr_find = 1\n",
    "if enable_lr_find:\n",
    "    print('LR plotting ...')\n",
    "    learn.lr_find()\n",
    "    learn.recorder.plot()\n",
    "    plt.savefig('lr_find.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "3db56b21206b303f5ef78632597d61ab9f21280b"
   },
   "outputs": [],
   "source": [
    "from fastai.callbacks import SaveModelCallback\n",
    "cb_save_model = SaveModelCallback(learn, every=\"epoch\", name=f\"siamese_cl\")\n",
    "#cb_siamese_validate = SiameseValidateCallback(learn, txlog)\n",
    "cbs = [cb_save_model]#, cb_siamese_validate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ac59fa95808d328cc7574d6be08cd745042a8799"
   },
   "outputs": [],
   "source": [
    "max_lr = 1e-3\n",
    "lrs = [max_lr/100, max_lr/10, max_lr]\n",
    "learn.fit_one_cycle(100, lrs, callbacks=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21e0ae2a6cc37dd5a53215cde165949eac5ec165"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd33bbeb2f61393ba0fa162e5c95359cd88e3aed"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b8151ae994dc88eee113caf9d445ba29f28bb88"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e0a4979ade8506e4f44eac86ac8fc81c2d79e22c"
   },
   "source": [
    "For training data, we are taking all the images except for the category - New Whale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a58ebdfaff7509d2d13a168e5d98637c7a79ecd5"
   },
   "outputs": [],
   "source": [
    "new_df = df[df['Id']!= 'new_whale']\n",
    "#new_df = new_df[new_df['sighting_count']>1]\n",
    "\n",
    "print('shape of data for training ',new_df.shape)\n",
    "new_df.drop(columns = ['sighting_count'] , inplace=True)\n",
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "db741211d7ac577fc34451c2e3bd61d3084668e0"
   },
   "outputs": [],
   "source": [
    "new_df.reset_index(inplace=True)\n",
    "new_df.drop(columns='index' , inplace=True)\n",
    "new_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9be02143d6b46c3cc2349e37eab4ee19c14d0c54"
   },
   "outputs": [],
   "source": [
    "new_df.to_records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "707d8411bc15534542795bca17922ac2c181f52e"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The data set we are using for training contains all images except of new whales.\n",
    "we don't require creating phase values for this  datasets , as duplicate images are very few.\n",
    "i am using index present in train.csv as the phase value as we can use it for indexing very easily \n",
    "\"\"\"\n",
    "\n",
    "tagged = dict([(p,w) for _,p,w in new_df.to_records()])\n",
    "h2ps = dict([(idx , p ) for   idx,p,w in new_df.to_records()])\n",
    "p2h   = dict([(p , idx) for idx , p , w in new_df.to_records()])\n",
    "h2p = h2ps.copy()\n",
    "join = tagged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "54daa4f1950bba3f42133287b7f530ebad9e5824"
   },
   "outputs": [],
   "source": [
    "def expand_path(p):\n",
    "    if isfile(train_path/p): \n",
    "        return train_path/p\n",
    "    if isfile(test_path/p): \n",
    "        return test_path/p\n",
    "    return p\n",
    "\n",
    "p2size = {}\n",
    "for p in tqdm_notebook(join):\n",
    "    size      = pil_image.open(expand_path(p)).size\n",
    "    p2size[p] = size\n",
    "len(p2size), list(p2size.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98e3ec25fcf68f22ca706ded4db56e3948263a1e"
   },
   "outputs": [],
   "source": [
    "## phase value for all categories except new whale\n",
    "h2ws = {}\n",
    "new_whale = 'new_whale'\n",
    "for p,w in tagged.items():\n",
    "    if w != new_whale: # Use only identified whales\n",
    "        h = p2h[p]\n",
    "        if h not in h2ws: h2ws[h] = []\n",
    "        if w not in h2ws[h]: h2ws[h].append(w)\n",
    "for h,ws in h2ws.items():\n",
    "    if len(ws) > 1:\n",
    "        h2ws[h] = sorted(ws)\n",
    "len(h2ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c411335bacdbbc578c2085e96a282dddaf31322"
   },
   "outputs": [],
   "source": [
    "## for each whale category, observe the associated phase values , \n",
    "##store all whale categories even the categories with just one image ( this is  different from martin's approach)\n",
    "\n",
    "w2hs = {}\n",
    "for h,ws in h2ws.items():\n",
    "    if len(ws) == 1: # Use only unambiguous pictures\n",
    "\n",
    "        w = ws[0]\n",
    "        if w not in w2hs: w2hs[w] = []\n",
    "        if h not in w2hs[w]: w2hs[w].append(h)\n",
    "for w,hs in w2hs.items():\n",
    "    #if len(hs) > 1:\n",
    "        w2hs[w] = sorted(hs)\n",
    "len(w2hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b570d288a34784230903e7245bc46973dfa2f1d"
   },
   "outputs": [],
   "source": [
    "len(h2ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21586ee1f6f5b964cce7c0f8fbc79f11c89d8f6d"
   },
   "outputs": [],
   "source": [
    "def read_raw_image(p):\n",
    "    img = pil_image.open(expand_path(p))\n",
    "    return img\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "583584e94ed126ccb4835c5b55431aee42236006"
   },
   "outputs": [],
   "source": [
    "train = [] # A list of  indices of images to be used in training data.\n",
    "for hs in w2hs.values():\n",
    "    if len(hs) >= 1:\n",
    "        train += hs\n",
    "random.shuffle(train)\n",
    "train_set = set(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f60c7328c1334c5f3aebb1f0643a82be1b92340"
   },
   "outputs": [],
   "source": [
    "## we have whales categories with phases(images) more than 1. shuffle the phase values now.\n",
    "w2ts = {} #Associate the image index from train to each whale id.\n",
    "for w,hs in w2hs.items():\n",
    "    for h in hs:\n",
    "        if h in train_set:\n",
    "            if w not in w2ts: w2ts[w] = []\n",
    "            if h not in w2ts[w]: w2ts[w].append(h)\n",
    "for w,ts in w2ts.items(): w2ts[w] = np.array(ts)\n",
    "## then again for each whale categories see how many images you have , \n",
    "## you are working with 5004 whale categories and 15697 images \n",
    "    \n",
    "    \n",
    "t2i = {} # The position in train of each training image id\n",
    "for i,t in enumerate(train): t2i[t] = i\n",
    "\n",
    "len(train),len(w2ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80ad7b4a9c36babb90d16d35bf1a3e2c32e6c592"
   },
   "outputs": [],
   "source": [
    "# from keras.utils import Sequence\n",
    "# import keras\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "import random\n",
    "#from keras import backend as K\n",
    "\n",
    "try:\n",
    "    from lapjv import lapjv\n",
    "    segment = False\n",
    "except ImportError:\n",
    "    print('Module lap not found, emulating with much slower scipy.optimize.linear_sum_assignment')\n",
    "    segment = True\n",
    "    from scipy.optimize import linear_sum_assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf012efe99894b7b3105b1212a648ce6e616e26c"
   },
   "source": [
    "Import functions from fast ai library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21b6f35a9217d969c35e6cbe6deadd2a91273611"
   },
   "outputs": [],
   "source": [
    "\n",
    "fn2label = {row[1].Image: row[1].Id for row in df.iterrows()}  #new_\n",
    "path2fn = lambda path: re.search('\\w*\\.jpg$', path).group(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4c181a8e94c8562c95b912b83b6d15441e1a1450"
   },
   "source": [
    "Creating dataset for all the training images. Because of some reason , i am not able to create validation set as well ( produces error while indexing from match and unmatch matrices. If someone is able to find the work arounf the help will be appreciated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8ee9b67e7f566f5a6171bc62fce7336f47d1ce8"
   },
   "outputs": [],
   "source": [
    "classes = df.Id.unique()\n",
    "data = (\n",
    "    ImageItemList  ##df[(df.Id != 'new_whale') & (df.sighting_count >1)]\n",
    "        .from_df( df[(df.Id != 'new_whale')], train_path, cols=['Image'])\n",
    "        .no_split()##split_by_valid_func(lambda path: path2fn(path) in val_fns) \n",
    "        .label_from_func(lambda path: fn2label[path2fn(path)] ,  classes=classes)\n",
    "        .add_test(ImageItemList.from_folder(test_path))\n",
    "        .transform(get_transforms(do_flip=False), size=SZ, resize_method=ResizeMethod.SQUISH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78f6fa8a1799126b9d77278bfef8fb296fe13365"
   },
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "print(len(data.train.x))\n",
    "print(len(data.valid.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "022ea0dcfd8c06c719818784bc1e650006d35864"
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "import random\n",
    "\n",
    "# First try to use lapjv Linear Assignment Problem solver as it is much faster.\n",
    "# At the time I am writing this, kaggle kernel with custom package fail to commit.\n",
    "# scipy can be used as a fallback, but it is too slow to run this kernel under the time limit\n",
    "# As a workaround, use scipy with data partitioning.\n",
    "# Because algorithm is O(n^3), small partitions are much faster, but not what produced the submitted solution\n",
    "try:\n",
    "    from lapjv import lapjv\n",
    "    segment = False\n",
    "except ImportError:\n",
    "    print('Module lap not found, emulating with much slower scipy.optimize.linear_sum_assignment')\n",
    "    segment = True\n",
    "    from scipy.optimize import linear_sum_assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0ac7970666823565a0e4d61ea962d6800c1a2f13"
   },
   "source": [
    "TwoImDataset creation is the part where I am trying to replicate 'TrainingData Class' from https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563/output\n",
    "For whale categories having just one images in training data , matching pair -  same image pair (A,A) . For other categories it creates a de arrangement.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "192c2491b34cd71b2dcdebf2a6a1114799e81c65"
   },
   "outputs": [],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cf28a71941aa7b2904f1d29d0829897eafc979b0"
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "def is_even(num): return num % 2 == 0\n",
    "\n",
    "class TwoImDataset(Dataset):\n",
    "    def __init__(self, ds, score, steps = 1000):\n",
    "        self.ds = ds\n",
    "        self.whale_ids = ds.y.items\n",
    "        self.steps =1000\n",
    "        self.score  = -score\n",
    "        for ts in w2ts.values():\n",
    "            idxs =  ts.copy() #[t2i[t] for t in ts]\n",
    "            #idxs = [i for i in  idxs if i <score.shape[0]]\n",
    "            for i in idxs:\n",
    "                for j in idxs:\n",
    "                    self.score[i,j] = 10000.0   # Set a large value for matching\n",
    "        self.epsilon = 1.0\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def set_epsilon(self, epsilon=1.0):\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 2 * len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prob = np.random.random()\n",
    "        choice = idx % 2 == 0\n",
    "        if choice:\n",
    "            idx //=2\n",
    "        else:\n",
    "            idx = (idx - 1) // 2\n",
    "        \n",
    "        if prob < self.epsilon: \n",
    "            return self.sample_random(idx, choice)\n",
    "        else:\n",
    "            return self.sample_policy(idx, choice)\n",
    "                \n",
    "    def sample_random(self, idx, choice):\n",
    "        if choice:\n",
    "            return self.sample_same(idx)\n",
    "        else: return self.sample_different(idx)\n",
    "\n",
    "    def sample_same(self, idx):\n",
    "        whale_id = self.whale_ids[idx]        \n",
    "        candidates = list(np.where(self.whale_ids == whale_id)[0])\n",
    "        candidates.remove(idx) # dropping our current whale - we don't want to compare against an identical image!\n",
    "        \n",
    "        if len(candidates) == 0: # oops, there is only a single whale with this id in the dataset\n",
    "            return self.sample_different(idx)\n",
    "        \n",
    "        np.random.shuffle(candidates)\n",
    "        return self.construct_example(self.ds[idx][0], self.ds[candidates[0]][0], 1)\n",
    "    \n",
    "    def sample_different(self, idx):\n",
    "        whale_id = self.whale_ids[idx]\n",
    "        candidates = list(np.where(self.whale_ids != whale_id)[0])\n",
    "        np.random.shuffle(candidates)\n",
    "        return self.construct_example(self.ds[idx][0], self.ds[candidates[0]][0], 0)\n",
    "  \n",
    "    def sample_policy(self, idx , tag):\n",
    "        #set_trace()\n",
    "        if tag==0:\n",
    "            first_image_id =  self.match[idx][0]\n",
    "            second_image_id = self.match[idx][1]\n",
    "            #if first_image_id < len(self.ds) and second_image_id< len(self.ds):         \n",
    "            return self.construct_example(self.ds[first_image_id][0], self.ds[second_image_id][0], 1)\n",
    "        else:\n",
    "            first_image_id =  self.unmatch[idx][0]\n",
    "            second_image_id = self.unmatch[idx][1]     \n",
    "            return self.construct_example(self.ds[first_image_id][0], self.ds[second_image_id][0], 0)\n",
    "  \n",
    "    def on_epoch_end(self):\n",
    "        if self.steps <= 0: return # Skip this on the last epoch.\n",
    "        self.steps     -= 1\n",
    "        self.match      = []\n",
    "        self.unmatch    = []\n",
    "        if segment:\n",
    "            tmp   = []\n",
    "            batch = 512\n",
    "            for start in range(0, score.shape[0], batch):\n",
    "                end = min(score.shape[0], start + batch)\n",
    "                _, x = linear_sum_assignment(self.score[start:end, start:end])\n",
    "                tmp.append(x + start)\n",
    "            x = np.concatenate(tmp)\n",
    "        else:\n",
    "            #print('using lapjv')\n",
    "            x,_,_ = lapjv(self.score) # Solve the linear assignment problem\n",
    "        y = np.arange(len(x),dtype=np.int32)\n",
    "\n",
    "        # Compute a derangement for matching whales\n",
    "        for ts in w2ts.values():\n",
    "            d = ts.copy()\n",
    "            if (len(d)==1):\n",
    "                for ab in zip(ts,d): self.match.append(ab)\n",
    "            else:                \n",
    "                while True:\n",
    "                    random.shuffle(d)\n",
    "                    if not np.any(ts == d): break\n",
    "                for ab in zip(ts,d): self.match.append(ab)\n",
    "\n",
    "        # Construct unmatched whale pairs from the LAP solution.\n",
    "        for i,j in zip(x,y):\n",
    "            if i == j:\n",
    "                print(self.score)\n",
    "                print(x)\n",
    "                print(y)\n",
    "                print(i,j)\n",
    "            assert i != j\n",
    "            self.unmatch.append((train[i],train[j]))\n",
    "\n",
    "        # Force a different choice for an eventual next epoch.\n",
    "        self.score[x,y] = 10000.0\n",
    "        self.score[y,x] = 10000.0\n",
    "        random.shuffle(self.match)\n",
    "        random.shuffle(self.unmatch)\n",
    "        print('end of epoch, math',self.match[0][0])\n",
    "        print('end of epoch, unmatch',self.unmatch[0][0])\n",
    "        \n",
    "        print(len(self.match), len(train), len(self.unmatch), len(train))\n",
    "        #assert len(self.match) == len(train) and len(self.unmatch) == len(train)\n",
    "    \n",
    "    def construct_example(self, im_A, im_B, class_idx):\n",
    "        return [im_A, im_B], class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c290bbc91b554d25d193cbbff88fea58cf2680da",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a 2D score matrix of size of training data\n",
    "\"\"\"\n",
    "\n",
    "score = np.random.random_sample(size=(len(train),len(train)))\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    TwoImDataset(data.train , score),\n",
    "    batch_size=BS,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d9f92c507d697957a48fc20f2653f1ce582ce673"
   },
   "outputs": [],
   "source": [
    "def normalize_batch(batch):\n",
    "    stat_tensors = [torch.tensor(l).cuda() for l in imagenet_stats]\n",
    "    return [normalize(batch[0][0], *stat_tensors), normalize(batch[0][1], *stat_tensors)], batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72f642d102e722b052ace01b5ae3791aeb98ea8f"
   },
   "outputs": [],
   "source": [
    "data_bunch = ImageDataBunch(train_dl , train_dl) ##, valid_dl\n",
    "data_bunch.add_tfm(normalize_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91e7aa7456f6b4d7bd81a898e33670b924defca2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The netowrk architecture is also inspired from Martin's notebook (part after we extract features for two image pairs)\n",
    "\"\"\"\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, arch=models.resnet50):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.cnn = create_body(arch)\n",
    "        self.head = nn.Linear(num_features_model(self.cnn), 1)  #\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1 , 32 , kernel_size= (1 , 4) , padding = 0 ,stride=1)\n",
    "        self.conv2 = nn.Conv2d( 1 , 1 , kernel_size = (32 ,1 ) , padding = 0  , stride=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, im_A, im_B):\n",
    "        x1, x2 = seq(im_A, im_B).map(self.cnn).map(self.process_features)\n",
    "        d1 = self.calculate_distance(x1, x2)\n",
    "        d2 = (x1 + x2)\n",
    "        d3 = (x1*x2)\n",
    "        d4 = (x1-x2)*(x1 - x2)\n",
    "        concat_layer = torch.cat([d1 ,d2,d3, d4]  ,dim = 1)\n",
    "        concat_layer = concat_layer.view( - 1, 1, num_features_model(self.cnn) , 4)   ## no of channels is second dimension\n",
    "        concat_layer  = F.relu(self.conv1(concat_layer))\n",
    "        concat_layer = concat_layer.view(-1 ,1,32, num_features_model(self.cnn))\n",
    "        concat_layer = F.relu(self.conv2(concat_layer))\n",
    "        concat_layer_fn = concat_layer.view(-1 ,num_features_model(self.cnn))\n",
    "        dropt = self.dropout(concat_layer_fn)\n",
    "        out = self.head(dropt)\n",
    "        return out\n",
    "    \n",
    "    def process_features(self, x): \n",
    "        y = x.reshape(*x.shape[:2], -1)\n",
    "        return x.reshape(*x.shape[:2], -1).max(-1)[0]\n",
    "    def calculate_distance(self, x1, x2): return (x1 - x2).abs_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "95bddab99cbd5025e7cf798a4207b77d94658581"
   },
   "outputs": [],
   "source": [
    "learn = Learner(data_bunch, SiameseNetwork(), \n",
    "                loss_func=BCEWithLogitsFlat(), \n",
    "                wd=0.01,\n",
    "                metrics=[lambda preds, targs: accuracy_thresh(preds.squeeze(), targs, sigmoid=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0dec12a3603df5f2d08642641e7f2dd19cb9fb06"
   },
   "outputs": [],
   "source": [
    "def linear_schedule(step, pars):\n",
    "    \"Linearly output value, end_step must greater than start_step\"\n",
    "    start_value = pars[0]\n",
    "    end_value = pars[1]\n",
    "    start_step = pars[2]\n",
    "    end_step = pars[3]\n",
    "    assert start_step <= end_step\n",
    "\n",
    "    if step < start_step:\n",
    "        return start_value\n",
    "    elif step >= end_step:\n",
    "        return end_value\n",
    "    return start_value - (step - start_step) * (start_value - end_value) / (end_step - start_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0fbe9f4bc37ea2d37e799db16548f26088b4529b"
   },
   "outputs": [],
   "source": [
    "linear_decay = partial(linear_schedule, pars=(1.0, 0.05, 2, 12))\n",
    "linear_decay(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f464689e671c475f34633b9ff3db822f5f6e9e56"
   },
   "outputs": [],
   "source": [
    "class DataloaderCallback(fastai.callbacks.tracker.TrackerCallback):\n",
    "    def __init__(self, learn, schedule_pars=(1.0, 0.05, 0, 10)):\n",
    "        super().__init__(learn)\n",
    "        self.schedule = partial(linear_schedule, pars=schedule_pars)\n",
    "        \n",
    "    #def on_batch_end(self, last_loss, epoch, num_batch, **kwargs: Any) -> None:\n",
    "    def on_epoch_begin(self, epoch, **kwargs: Any) -> None:\n",
    "        epsilon = self.schedule(epoch)\n",
    "        self.learn.data.train_ds.set_epsilon(epsilon)\n",
    "        self.learn.data.train_ds.on_epoch_end()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e40e29e94dac856490f179f7637080e0c9bc5365"
   },
   "outputs": [],
   "source": [
    "learn.split([learn.model.cnn[:6], learn.model.cnn[6:], learn.model.head])\n",
    "learn.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f63223ee2bfa9d05457908b28e59bcf2c904041",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d6e91265fc44a500cba5b98c8df863acc3ca8b41"
   },
   "outputs": [],
   "source": [
    "cb_dl = DataloaderCallback(learn, schedule_pars=(1.0, 0.05, 0, 10))\n",
    "cbs = [cb_dl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8869bf0a838bb863ce423769d02371ecaccd508e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(4 , 1e-3, callbacks=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "16f717c0127af50991cfaa3da28a5d3b945c1684"
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa264f42b8195885f1f388445d126700b0bddd29"
   },
   "outputs": [],
   "source": [
    "max_lr = 3e-4\n",
    "lrs = [max_lr/100, max_lr/10, max_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "11a27543aac02d1829f5cfce2f2a3bd3f63aeed7"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage2_unfz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "45b00cce51bd3f20d3a06e670dcb0887e504a96e"
   },
   "outputs": [],
   "source": [
    "learn.load(f'{name}-stage2_unfz');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7db7ce34c70ddc1712db64bfb9f92f6ab09ad62e"
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31d68aa57fe121d6ff2ea336c34ad3c722b5a70d"
   },
   "outputs": [],
   "source": [
    "cb_dl = DataloaderCallback(learn, schedule_pars=(0.05, 0.01, 0, 10))\n",
    "cbs = [cb_dl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "84365bc64ee9a0721ac1388eaa8175e1ba39248f"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage3_unfz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0c19280da6a71017bdde5adc21b8fddce0c2b77e"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage4_unfz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "907d93f72c9195f343defb70ba2097ee2fd1e7a8"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage5_unfz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cfda61725a5a9a82937970e4f75211d1f295b6c9"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage6_unfz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "718a3c7561aa51e5d7902c718b0fcc6a1f930da7"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage7_unfz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "efa1effc2bbed8d07aba9b913552ded9b4d5d7b3"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage8_unfz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f49bd41d195adb34deeb66e0617c48237679810"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage9_unfz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "83e090f8278044116508d8ca6c8676fd73f145d6"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs, callbacks=cbs)\n",
    "learn.save(f'{name}-stage10_unfz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d798d591ea9f4d5c5a14a99b752bc038de6c192d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4e9b4695761c9b282516f1cb4714add490050d7d"
   },
   "outputs": [],
   "source": [
    "learn.load(f'{name}-stage9_unfz')\n",
    "learn.model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6dbddfedb90054e189326aba0ccf42aecf7f9ecf"
   },
   "outputs": [],
   "source": [
    "new_whale_fns = set(df[df['Id']=='new_whale'].sample(frac = 1).Image.iloc[:1000])\n",
    "#new_whale_fns\n",
    "val_fns = set(df[df.sighting_count == 2].Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0508f2be224c3220ffa2a0c41bc8c7dfeb8ff302"
   },
   "outputs": [],
   "source": [
    "val_fns = set(df[df.sighting_count == 2].Image)\n",
    "print(len(val_fns) + len(new_whale_fns))\n",
    "\n",
    "classes = df.Id.unique()\n",
    "\n",
    "df = df.drop(columns = ['sighting_count'])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2120ab5ab3dcacf59852825ab673f227cfa40fad"
   },
   "outputs": [],
   "source": [
    "\n",
    "data = (\n",
    "    ImageItemList\n",
    "        .from_df(df, train_path, cols=['Image'])\n",
    "        .split_by_valid_func(lambda path: path2fn(path) in val_fns.union(new_whale_fns))\n",
    "        .label_from_func(lambda path: fn2label[path2fn(path)], classes=classes)\n",
    "        .add_test(ImageItemList.from_folder(test_path))\n",
    "        .transform(get_transforms(do_flip=False), size=SZ, resize_method=ResizeMethod.SQUISH)\n",
    "        .databunch(bs=BS, num_workers=NUM_WORKERS, path=root_path)\n",
    "        .normalize(imagenet_stats)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a87303524c041736fd35e52d22cbd59804b5872",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dist_mat, val_target, _ = cal_mat(learn.model, data.valid_dl, data.valid_dl, ds_with_target1=True, ds_with_target2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "56ce8c0977654009fd3b0e110d3bd1329544ff24"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "targs = []\n",
    "feats = []\n",
    "learn.model.eval()\n",
    "with torch.no_grad():\n",
    "    for ims, ts in data.valid_dl:\n",
    "        feats.append(learn.model.process_features(learn.model.cnn(ims)))  ##\n",
    "        targs.append(ts)\n",
    "\n",
    "    feats = torch.cat(feats)\n",
    "    print(feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "56cf455037a951cbb2cd9b50274ab22c1db8917c"
   },
   "outputs": [],
   "source": [
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fc47dbfadf97ce95fea38071c670f616fe8ad9a4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sims = []\n",
    "learn.model.eval()\n",
    "with torch.no_grad():\n",
    "    for feat in feats:\n",
    "        x1 = feats#.copy()\n",
    "        x2 = feat.unsqueeze(0).repeat(3570 ,1)\n",
    "        d1 = learn.model.calculate_distance(x1 , x2)\n",
    "        d2 = (x1 + x2)\n",
    "        d3 = (x1*x2)\n",
    "        d4 = (x1-x2)*(x1 - x2)\n",
    "        concat_layer = torch.cat([d1 ,d2,d3, d4]  ,dim = 1)\n",
    "        concat_layer = concat_layer.view( - 1, 1, num_features_model(learn.model.cnn) , 4)   ## no of channels is second dimension\n",
    "        concat_layer  = F.relu(learn.model.conv1(concat_layer.cuda()))\n",
    "        concat_layer = concat_layer.view(-1 ,1,32, num_features_model(learn.model.cnn)  )\n",
    "        concat_layer = F.relu(learn.model.conv2(concat_layer))\n",
    "        concat_layer_fn = concat_layer.view(-1 ,num_features_model(learn.model.cnn) )\n",
    "        #out = learn.model.head(concat_layer_fn)\n",
    "        predicted_similarity = learn.model.head(concat_layer_fn).sigmoid_()  #.cuda()\n",
    "        sims.append(predicted_similarity.squeeze())\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b5fb7e323df740b955cff3cd6fe9200c187bc19a"
   },
   "outputs": [],
   "source": [
    "new_whale_idx = np.where(classes == 'new_whale')[0][0]\n",
    "new_whale_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff744445903289ee7082fc6dc714b313eea02ecf"
   },
   "outputs": [],
   "source": [
    "top_5s = []\n",
    "learn.model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, sim in enumerate(sims):\n",
    "        idxs = sim.argsort(descending=True)\n",
    "        probs = sim[idxs]\n",
    "        top_5 = []\n",
    "        for j, p in zip(idxs, probs):\n",
    "            if len(top_5) == 5: break\n",
    "            if j == i: continue   \n",
    "            predicted_class = data.valid_ds.y.items[j]\n",
    "            \"\"\"\n",
    "            we dont want to predict new whale for validation data \n",
    "            \"\"\"\n",
    "            if predicted_class == new_whale_idx: continue\n",
    "            if predicted_class not in top_5: top_5.append(predicted_class)\n",
    "        top_5s.append(top_5)\n",
    "\n",
    "    ## top 5 contains 5 best predicted classes ,w ith indices from classes dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04b660a7e902fee06e944ae1ca7dc4900844f500"
   },
   "outputs": [],
   "source": [
    "top_5s[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92dea147d6f02f252d1c35be1becd130f5c547e2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mapk of validation data set without having new whales in predictions. \n",
    "\"\"\"\n",
    "mapk(data.valid_ds.y.items.reshape(-1,1), np.stack(top_5s), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1acd32b8060fbdd668888b45cd9fb3fed3033f6f"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\"\"\"\n",
    "trying to calcualte threshold probability for new whale, which maximises the mapk for validation data.\n",
    "\"\"\"\n",
    "learn.model.eval()\n",
    "with torch.no_grad():\n",
    "    for thresh in np.linspace(0.99, 1, 10):\n",
    "        top_5s = []\n",
    "        for i, sim in enumerate(sims):\n",
    "            idxs = sim.argsort(descending=True)\n",
    "            probs = sim[idxs]\n",
    "            top_5 = []\n",
    "            for j, p in zip(idxs, probs):\n",
    "                if new_whale_idx not in top_5 and p < thresh and len(top_5) < 5: top_5.append(new_whale_idx)\n",
    "                if len(top_5) == 5: break\n",
    "                if j == i: continue\n",
    "                predicted_class = data.valid_ds.y.items[j]\n",
    "                if predicted_class not in top_5: top_5.append(predicted_class)\n",
    "            top_5s.append(top_5)\n",
    "        print(thresh, mapk(data.valid_ds.y.items.reshape(-1,1), np.stack(top_5s), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eefa2f7b2e3c4f0bfad4720933105bdba191af79"
   },
   "outputs": [],
   "source": [
    "data = (\n",
    "    ImageItemList\n",
    "        .from_df(df, train_path, cols=['Image'])\n",
    "        .split_by_valid_func(lambda path: path2fn(path) in {'69823499d.jpg'}) # in newer version of the fastai library there is .no_split that could be used here\n",
    "        .label_from_func(lambda path: fn2label[path2fn(path)], classes=classes)\n",
    "        .add_test(ImageItemList.from_folder(test_path))\n",
    "        .transform(None, size=SZ, resize_method=ResizeMethod.SQUISH)\n",
    "        .databunch(bs=BS, num_workers=NUM_WORKERS, path=root_path)\n",
    "        .normalize(imagenet_stats)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c7ab39a1244ec5dc0153497a181d0c914113e4f"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "test_feats = []\n",
    "learn.model.eval()\n",
    "with torch.no_grad():\n",
    "    for ims, _ in data.test_dl:\n",
    "        test_feats.append(learn.model.process_features(learn.model.cnn(ims)))\n",
    "\n",
    "\n",
    "    test_feats = torch.cat(test_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f0a0c68cf627b5d6496ca1d390e81acd6493b7ba"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_feats = []\n",
    "train_class_idxs = []\n",
    "learn.model.eval()\n",
    "with torch.no_grad():\n",
    "    for ims, t in data.train_dl:\n",
    "        train_feats.append(learn.model.process_features(learn.model.cnn(ims)))\n",
    "        train_class_idxs.append(t)\n",
    "\n",
    "    train_class_idxs = torch.cat(train_class_idxs)\n",
    "    train_feats = torch.cat(train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5917229d93cfad6db0183e95e7d86b87ea370f00"
   },
   "outputs": [],
   "source": [
    "len(train_class_idxs)\n",
    "len(train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f8bd6f2960ae496ba7af48a227b46aab4318ce0c"
   },
   "outputs": [],
   "source": [
    "train_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9113c29f41ba0b69e41ef750e0c6da0e50a9775b"
   },
   "outputs": [],
   "source": [
    "test_feats[0].expand(len(train_feats), 2048).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce62aee1e8260de36b85965410fa1c14abb79de6"
   },
   "outputs": [],
   "source": [
    "train_feats = train_feats.cuda()\n",
    "test_feats = test_feats.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4823e11f48d1300171d331126586a9e58c31b8f0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee0a098b7de6f941ab079878c0544d27bad599d7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f768b5db3aa245c5bb13d9a6716312c8b7d5770"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.cuda.empty_cache()\n",
    "sims = []\n",
    "learn.model.eval()\n",
    "with torch.no_grad():\n",
    "    tmp_batch_size = 1000\n",
    "    for i, feat in enumerate(test_feats):\n",
    "        if i % 1000 == 0:\n",
    "            print(i, len(test_feats))\n",
    "        #dists = learn.model.calculate_distance(train_feats, feat.unsqueeze(0).repeat(len(train_feats), 1))\n",
    "        row_sim = []\n",
    "        for k in range(0, len(train_feats), tmp_batch_size):\n",
    "            x1 = train_feats[k:k+tmp_batch_size]\n",
    "            x2 = feat.expand(x1.shape)\n",
    "            d1 = learn.model.calculate_distance(x1 , x2)\n",
    "            d2 = (x1 + x2)\n",
    "            d3 = (x1*x2)\n",
    "            d4 = (x1-x2)*(x1 - x2)\n",
    "            concat_layer = torch.cat([d1 ,d2,d3, d4]  ,dim = 1)\n",
    "            concat_layer = concat_layer.view( - 1, 1, num_features_model(learn.model.cnn) , 4)   ## no of channels is second dimension\n",
    "            concat_layer  = F.relu(learn.model.conv1(concat_layer.cuda()))\n",
    "            concat_layer = concat_layer.view(-1 ,1,32, num_features_model(learn.model.cnn)  )\n",
    "            concat_layer = F.relu(learn.model.conv2(concat_layer))\n",
    "            concat_layer_fn = concat_layer.view(-1 ,num_features_model(learn.model.cnn) )\n",
    "            predicted_similarity = learn.model.head(concat_layer_fn).sigmoid_()  #.cuda()\n",
    "            row_sim.append(predicted_similarity)\n",
    "        row_sim = torch.cat(row_sim)\n",
    "        sims.append(row_sim.squeeze().detach().cpu())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4bd768e7084208b028e4019a5092703f2932ec9d"
   },
   "outputs": [],
   "source": [
    "\n",
    "thresh = 0.996 #0.95\n",
    "\n",
    "top_5s = []\n",
    "\n",
    "for sim in sims:\n",
    "    idxs = sim.argsort(descending = True)\n",
    "    probs = sim[idxs]\n",
    "    top_5 = []\n",
    "    \n",
    "    \n",
    "    for  i , p in zip(idxs , probs):\n",
    "        if new_whale_idx not in top_5 and p < thresh  and len(top_5) < 5:\n",
    "            top_5.append(new_whale_idx)\n",
    "        if len(top_5) ==5: break\n",
    "        #if i == new_whale_idx: continue\n",
    "        predicted_class = train_class_idxs[i]\n",
    "        #print(predicted_class)\n",
    "        if predicted_class == new_whale_idx: continue\n",
    "        if predicted_class not in top_5:\n",
    "            top_5.append(predicted_class)\n",
    "    top_5s.append(top_5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1d5990f0330439a7b24e5ab05eb27f2f05843929"
   },
   "outputs": [],
   "source": [
    "len(top_5s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9217825957a831d86bb8d3de82a2f128c56fdc6b"
   },
   "outputs": [],
   "source": [
    "top_5_classes  = []\n",
    "\n",
    "for top_5 in top_5s:\n",
    "    top_5_classes.append(' '.join([classes[t] for t in top_5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f0d409a2f06b813e3a80054c2403b66854bb2a89"
   },
   "outputs": [],
   "source": [
    "top_5_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "573cee7e98b06a636f0d14056331b6f4511f80f7"
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'Image': [path.name for path in data.test_ds.x.items]})\n",
    "sub['Id'] = top_5_classes\n",
    "sub.to_csv(f'../submission/{name}.csv', index=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a82e26f1fee6c232927d2542e366405af70e1984"
   },
   "outputs": [],
   "source": [
    "pd.read_csv(f'../submission/{name}.csv').Id.str.split().apply(lambda x: x[0] == 'new_whale').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b7d058c20a929de90d70c9ed2101d9d05464a280"
   },
   "outputs": [],
   "source": [
    "#name = 'Ensembleing_resnet50_renet101_siamene_v1'\n",
    "! kaggle competitions submit -c humpback-whale-identification -f ../submission/martin_siamene_network_15k_training_images.csv -m \"resnet18 arch prob 1 for new whales\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ad38603d8296581280edf81adcb6cce8d255d5e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
